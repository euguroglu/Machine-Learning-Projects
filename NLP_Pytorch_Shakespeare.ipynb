{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Pytorch_Shakespeare.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxlwcbhLomLlqMC2dJAyQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euguroglu/Machine-Learning-Projects/blob/master/NLP_Pytorch_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3fXvFOvUO_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5L8enMnUwfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('shakespeare.txt','r',encoding='utf8') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2u12leYVDdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "459850d1-fb3e-4ed3-fc38-54299685278c"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsxRvUipVS6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "846cfd73-4856-4eb0-d6bd-b983b8ce3004"
      },
      "source": [
        "text[:1000]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2p8goy-Vcjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "7a4a3991-3270-4210-ddc9-e85c4c3ebcb5"
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9vmQFNvVjl5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7661ef76-ec85-4b18-8e0c-13c5f2f1de24"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwUbMLvpVluh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0CBs-KmVryg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5c2cc37-618c-45e9-d25e-5ad5189b28b0"
      },
      "source": [
        "all_characters"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '>',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6oD58GFVsxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c1caaf76-3cce-47fd-af97-d39c5c31d815"
      },
      "source": [
        "len(all_characters)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfIwJRDXXaYD",
        "colab_type": "text"
      },
      "source": [
        "Decoder takes number and transform it into letter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAcd31h5VwVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDun8XaEX7aL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "774dae75-338e-41a4-d3e4-027ec3cbb62c"
      },
      "source": [
        "decoder"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'b',\n",
              " 1: '9',\n",
              " 2: 'w',\n",
              " 3: '[',\n",
              " 4: 'W',\n",
              " 5: 'P',\n",
              " 6: 'a',\n",
              " 7: 'B',\n",
              " 8: 'S',\n",
              " 9: 'x',\n",
              " 10: 'n',\n",
              " 11: 'k',\n",
              " 12: 'H',\n",
              " 13: '!',\n",
              " 14: 'C',\n",
              " 15: 'K',\n",
              " 16: 'r',\n",
              " 17: '|',\n",
              " 18: 'u',\n",
              " 19: ':',\n",
              " 20: 'J',\n",
              " 21: 'j',\n",
              " 22: '2',\n",
              " 23: '7',\n",
              " 24: 't',\n",
              " 25: '&',\n",
              " 26: 'd',\n",
              " 27: '`',\n",
              " 28: '8',\n",
              " 29: ',',\n",
              " 30: \"'\",\n",
              " 31: '6',\n",
              " 32: 'l',\n",
              " 33: '.',\n",
              " 34: '<',\n",
              " 35: ']',\n",
              " 36: ';',\n",
              " 37: 'T',\n",
              " 38: '3',\n",
              " 39: '_',\n",
              " 40: 'A',\n",
              " 41: 'y',\n",
              " 42: 'L',\n",
              " 43: '5',\n",
              " 44: 'V',\n",
              " 45: 'e',\n",
              " 46: 'Q',\n",
              " 47: 'N',\n",
              " 48: 'p',\n",
              " 49: '4',\n",
              " 50: 'z',\n",
              " 51: '?',\n",
              " 52: ' ',\n",
              " 53: 'h',\n",
              " 54: 'D',\n",
              " 55: 'U',\n",
              " 56: ')',\n",
              " 57: 's',\n",
              " 58: 'F',\n",
              " 59: 'v',\n",
              " 60: '(',\n",
              " 61: '0',\n",
              " 62: 'G',\n",
              " 63: 'X',\n",
              " 64: '>',\n",
              " 65: 'c',\n",
              " 66: 'Z',\n",
              " 67: 'E',\n",
              " 68: '}',\n",
              " 69: 'i',\n",
              " 70: 'R',\n",
              " 71: 'M',\n",
              " 72: 'I',\n",
              " 73: '-',\n",
              " 74: 'g',\n",
              " 75: 'q',\n",
              " 76: '\"',\n",
              " 77: '1',\n",
              " 78: 'm',\n",
              " 79: '\\n',\n",
              " 80: 'f',\n",
              " 81: 'Y',\n",
              " 82: 'O',\n",
              " 83: 'o'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lygnkSNX74J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = {char: ind for ind,char in decoder.items()}"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ0OQmIeYOOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afe36680-0451-473a-c00a-4f521c3a97d8"
      },
      "source": [
        "encoder"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 79,\n",
              " ' ': 52,\n",
              " '!': 13,\n",
              " '\"': 76,\n",
              " '&': 25,\n",
              " \"'\": 30,\n",
              " '(': 60,\n",
              " ')': 56,\n",
              " ',': 29,\n",
              " '-': 73,\n",
              " '.': 33,\n",
              " '0': 61,\n",
              " '1': 77,\n",
              " '2': 22,\n",
              " '3': 38,\n",
              " '4': 49,\n",
              " '5': 43,\n",
              " '6': 31,\n",
              " '7': 23,\n",
              " '8': 28,\n",
              " '9': 1,\n",
              " ':': 19,\n",
              " ';': 36,\n",
              " '<': 34,\n",
              " '>': 64,\n",
              " '?': 51,\n",
              " 'A': 40,\n",
              " 'B': 7,\n",
              " 'C': 14,\n",
              " 'D': 54,\n",
              " 'E': 67,\n",
              " 'F': 58,\n",
              " 'G': 62,\n",
              " 'H': 12,\n",
              " 'I': 72,\n",
              " 'J': 20,\n",
              " 'K': 15,\n",
              " 'L': 42,\n",
              " 'M': 71,\n",
              " 'N': 47,\n",
              " 'O': 82,\n",
              " 'P': 5,\n",
              " 'Q': 46,\n",
              " 'R': 70,\n",
              " 'S': 8,\n",
              " 'T': 37,\n",
              " 'U': 55,\n",
              " 'V': 44,\n",
              " 'W': 4,\n",
              " 'X': 63,\n",
              " 'Y': 81,\n",
              " 'Z': 66,\n",
              " '[': 3,\n",
              " ']': 35,\n",
              " '_': 39,\n",
              " '`': 27,\n",
              " 'a': 6,\n",
              " 'b': 0,\n",
              " 'c': 65,\n",
              " 'd': 26,\n",
              " 'e': 45,\n",
              " 'f': 80,\n",
              " 'g': 74,\n",
              " 'h': 53,\n",
              " 'i': 69,\n",
              " 'j': 21,\n",
              " 'k': 11,\n",
              " 'l': 32,\n",
              " 'm': 78,\n",
              " 'n': 10,\n",
              " 'o': 83,\n",
              " 'p': 48,\n",
              " 'q': 75,\n",
              " 'r': 16,\n",
              " 's': 57,\n",
              " 't': 24,\n",
              " 'u': 18,\n",
              " 'v': 59,\n",
              " 'w': 2,\n",
              " 'x': 9,\n",
              " 'y': 41,\n",
              " 'z': 50,\n",
              " '|': 17,\n",
              " '}': 68}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ROQay0PYaGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojOh4E7IZjv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "c25ee7a2-d328-460e-cbf8-a96a2ebb02b9"
      },
      "source": [
        "encoded_text[:500]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([79, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52,\n",
              "       52, 52, 52, 52, 52, 77, 79, 52, 52, 58, 16, 83, 78, 52, 80,  6, 69,\n",
              "       16, 45, 57, 24, 52, 65, 16, 45,  6, 24, 18, 16, 45, 57, 52,  2, 45,\n",
              "       52, 26, 45, 57, 69, 16, 45, 52, 69, 10, 65, 16, 45,  6, 57, 45, 29,\n",
              "       79, 52, 52, 37, 53,  6, 24, 52, 24, 53, 45, 16, 45,  0, 41, 52,  0,\n",
              "       45,  6, 18, 24, 41, 30, 57, 52, 16, 83, 57, 45, 52, 78, 69, 74, 53,\n",
              "       24, 52, 10, 45, 59, 45, 16, 52, 26, 69, 45, 29, 79, 52, 52,  7, 18,\n",
              "       24, 52,  6, 57, 52, 24, 53, 45, 52, 16, 69, 48, 45, 16, 52, 57, 53,\n",
              "       83, 18, 32, 26, 52,  0, 41, 52, 24, 69, 78, 45, 52, 26, 45, 65, 45,\n",
              "        6, 57, 45, 29, 79, 52, 52, 12, 69, 57, 52, 24, 45, 10, 26, 45, 16,\n",
              "       52, 53, 45, 69, 16, 52, 78, 69, 74, 53, 24, 52,  0, 45,  6, 16, 52,\n",
              "       53, 69, 57, 52, 78, 45, 78, 83, 16, 41, 19, 79, 52, 52,  7, 18, 24,\n",
              "       52, 24, 53, 83, 18, 52, 65, 83, 10, 24, 16,  6, 65, 24, 45, 26, 52,\n",
              "       24, 83, 52, 24, 53, 69, 10, 45, 52, 83,  2, 10, 52,  0, 16, 69, 74,\n",
              "       53, 24, 52, 45, 41, 45, 57, 29, 79, 52, 52, 58, 45, 45, 26, 30, 57,\n",
              "       24, 52, 24, 53, 41, 52, 32, 69, 74, 53, 24, 30, 57, 52, 80, 32,  6,\n",
              "       78, 45, 52,  2, 69, 24, 53, 52, 57, 45, 32, 80, 73, 57, 18,  0, 57,\n",
              "       24,  6, 10, 24, 69,  6, 32, 52, 80, 18, 45, 32, 29, 79, 52, 52, 71,\n",
              "        6, 11, 69, 10, 74, 52,  6, 52, 80,  6, 78, 69, 10, 45, 52,  2, 53,\n",
              "       45, 16, 45, 52,  6,  0, 18, 10, 26,  6, 10, 65, 45, 52, 32, 69, 45,\n",
              "       57, 29, 79, 52, 52, 37, 53, 41, 52, 57, 45, 32, 80, 52, 24, 53, 41,\n",
              "       52, 80, 83, 45, 29, 52, 24, 83, 52, 24, 53, 41, 52, 57,  2, 45, 45,\n",
              "       24, 52, 57, 45, 32, 80, 52, 24, 83, 83, 52, 65, 16, 18, 45, 32, 19,\n",
              "       79, 52, 52, 37, 53, 83, 18, 52, 24, 53,  6, 24, 52,  6, 16, 24, 52,\n",
              "       10, 83,  2, 52, 24, 53, 45, 52,  2, 83, 16, 32, 26, 30, 57, 52, 80,\n",
              "       16, 45, 57, 53, 52, 83, 16, 10,  6, 78, 45, 10, 24, 29, 79, 52, 52,\n",
              "       40, 10, 26, 52, 83, 10, 32, 41, 52, 53, 45, 16,  6, 32, 26, 52, 24,\n",
              "       83, 52, 24, 53, 45, 52, 74,  6, 18, 26, 41, 52, 57, 48, 16, 69, 10,\n",
              "       74, 29, 79, 52, 52,  4, 69, 24, 53, 69, 10, 52, 24, 53, 69, 10, 45,\n",
              "       52, 83,  2, 10, 52,  0, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnNnn-CbZnKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e0f2d68-3353-4bf0-8eba-32ca58d1800d"
      },
      "source": [
        "decoder[9] #space"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'x'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_3dWJq-bFFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text,num_uni_chars):\n",
        "\n",
        "  # encoded text --> batch of encoded text\n",
        "  # num_uni_chars --> len(set(text))\n",
        "\n",
        "  one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
        "  one_hot = one_hot.astype(np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
        "  one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
        "\n",
        "  return one_hot\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUyoE8oueGko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e90707f4-a617-40af-b257-3078dff3fe35"
      },
      "source": [
        "one_hot_encoder(np.array([1,2,0]),3)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtQyU_hywsfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_Text = np.arange(10)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgjvg2KSxemN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8804a85-45ef-457d-9c08-54f0b11c54b4"
      },
      "source": [
        "example_Text"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNACWWDhxf5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "bd7b6929-85ef-48f8-e051-a40bc6ef068f"
      },
      "source": [
        "example_Text.reshape((5,-1))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7],\n",
              "       [8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DgMzZsbxklk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(encoded_text,samp_per_batch=10,seq_len=50):\n",
        "    # Total number of characters per batch\n",
        "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
        "    # characters come out per batch.\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "    \n",
        "    \n",
        "    # Number of batches available to make\n",
        "    # Use int() to roun to nearest integer\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "    \n",
        "    # Cut off end of encoded_text that\n",
        "    # won't fit evenly into a batch\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "    \n",
        "    \n",
        "    # Reshape text into rows the size of a batch\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "    \n",
        "\n",
        "    # Go through each row in array.\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "        \n",
        "        # Grab feature characters\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "        \n",
        "        # y is the target shifted over by 1\n",
        "        y = np.zeros_like(x)\n",
        "       \n",
        "        #\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "            \n",
        "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "            \n",
        "        yield x, y"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4fHowSzTGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_text = np.arange(20)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLwDw5mu2hXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a4c7e305-201e-4a51-fd94-976481cfb466"
      },
      "source": [
        "sample_text"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBNPnSA2ie6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgtXHH0q2nRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = next(batch_generator)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL2nuVJG2omj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0daa9c76-d810-4938-8b69-703e029841cc"
      },
      "source": [
        "x"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4],\n",
              "       [10, 11, 12, 13, 14]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvOJ-naV87iU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a20810b-b112-4d82-aed5-9d37888656c7"
      },
      "source": [
        "y"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4,  5],\n",
              "       [11, 12, 13, 14, 15]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaPXUzrU876C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "        \n",
        "        \n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "        \n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                  \n",
        "        \n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        \n",
        "        drop_output = self.dropout(lstm_output)\n",
        "        \n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "        \n",
        "        \n",
        "        final_out = self.fc_linear(drop_output)\n",
        "        \n",
        "        \n",
        "        return final_out, hidden\n",
        "    \n",
        "    \n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "        \n",
        "        if self.use_gpu:\n",
        "            \n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "        \n",
        "        return hidden\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qWLe3gJCwd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvKlFNuKDUPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOwrmrdAD7DQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6ec7078b-7feb-4d02-cd23-92a0137fdef4"
      },
      "source": [
        "total_param"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[172032,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 1048576,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 1048576,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 43008,\n",
              " 84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPecJZnAEGNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6niMZ8yFEYVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent = 0.1"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl9sYf62EaAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ind = int(len(encoded_text)*train_percent)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttEMT6WdEd7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = encoded_text[:train_ind]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z18_YVCFEhBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EuqZhLyEjgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37c2a4bd-ee6a-4bbf-e16c-b7fe3cb3afeb"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3UbwA9cElAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d07a1d87-b2b7-469c-d8e1-377208970d85"
      },
      "source": [
        "len(val_data)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4901049"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW7vrOP8El1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent = 0.9"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GFdF169EnKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ind = int(len(encoded_text)*train_percent)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw7EEGfEEssv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = encoded_text[:train_ind]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3mvpM_VEtle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLStEpGEuXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c319bad-7cbd-43f6-b3fa-3c903d84effe"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4901048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZbCnBN4EvbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "598fa64d-d56e-4065-b870-25f146368409"
      },
      "source": [
        "len(val_data)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544561"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3xzKSXnEwdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 60\n",
        "batch_size = 100\n",
        "seq_len = 100\n",
        "tracker = 0\n",
        "num_char = max(encoded_text)+1\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4LMpRuPFG03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e26ed7b4-0475-4f5d-82e1-da0ec628b398"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZkGlfY_FH7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c89e642e-b453-4cf4-98be-c666a82bf590"
      },
      "source": [
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "    \n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "        \n",
        "        tracker += 1\n",
        "        \n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        \n",
        "        # Convert Numpy Arrays to Tensor\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "        \n",
        "        # Adjust for GPU if necessary\n",
        "        \n",
        "        if model.use_gpu:\n",
        "            \n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "        \n",
        "        if tracker % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "                \n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "                \n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through \n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.2024028301239014\n",
            "Epoch: 0 Step: 50 Val Loss: 3.193345785140991\n",
            "Epoch: 0 Step: 75 Val Loss: 3.189502477645874\n",
            "Epoch: 0 Step: 100 Val Loss: 3.1815578937530518\n",
            "Epoch: 0 Step: 125 Val Loss: 3.0615179538726807\n",
            "Epoch: 0 Step: 150 Val Loss: 2.9390108585357666\n",
            "Epoch: 0 Step: 175 Val Loss: 2.81571364402771\n",
            "Epoch: 0 Step: 200 Val Loss: 2.688997983932495\n",
            "Epoch: 0 Step: 225 Val Loss: 2.5762393474578857\n",
            "Epoch: 0 Step: 250 Val Loss: 2.425086736679077\n",
            "Epoch: 0 Step: 275 Val Loss: 2.310821294784546\n",
            "Epoch: 0 Step: 300 Val Loss: 2.2313435077667236\n",
            "Epoch: 0 Step: 325 Val Loss: 2.1682653427124023\n",
            "Epoch: 0 Step: 350 Val Loss: 2.1150121688842773\n",
            "Epoch: 0 Step: 375 Val Loss: 2.073878765106201\n",
            "Epoch: 0 Step: 400 Val Loss: 2.0400619506835938\n",
            "Epoch: 0 Step: 425 Val Loss: 2.0077548027038574\n",
            "Epoch: 0 Step: 450 Val Loss: 1.9958865642547607\n",
            "Epoch: 0 Step: 475 Val Loss: 1.9546757936477661\n",
            "Epoch: 1 Step: 500 Val Loss: 1.9355239868164062\n",
            "Epoch: 1 Step: 525 Val Loss: 1.910528302192688\n",
            "Epoch: 1 Step: 550 Val Loss: 1.8854353427886963\n",
            "Epoch: 1 Step: 575 Val Loss: 1.8661620616912842\n",
            "Epoch: 1 Step: 600 Val Loss: 1.845189094543457\n",
            "Epoch: 1 Step: 625 Val Loss: 1.830608606338501\n",
            "Epoch: 1 Step: 650 Val Loss: 1.8038452863693237\n",
            "Epoch: 1 Step: 675 Val Loss: 1.792361855506897\n",
            "Epoch: 1 Step: 700 Val Loss: 1.776977777481079\n",
            "Epoch: 1 Step: 725 Val Loss: 1.7622116804122925\n",
            "Epoch: 1 Step: 750 Val Loss: 1.7480789422988892\n",
            "Epoch: 1 Step: 775 Val Loss: 1.7359503507614136\n",
            "Epoch: 1 Step: 800 Val Loss: 1.7291617393493652\n",
            "Epoch: 1 Step: 825 Val Loss: 1.7127195596694946\n",
            "Epoch: 1 Step: 850 Val Loss: 1.7027207612991333\n",
            "Epoch: 1 Step: 875 Val Loss: 1.690297245979309\n",
            "Epoch: 1 Step: 900 Val Loss: 1.6815310716629028\n",
            "Epoch: 1 Step: 925 Val Loss: 1.6698955297470093\n",
            "Epoch: 1 Step: 950 Val Loss: 1.6615357398986816\n",
            "Epoch: 1 Step: 975 Val Loss: 1.6487407684326172\n",
            "Epoch: 2 Step: 1000 Val Loss: 1.6468546390533447\n",
            "Epoch: 2 Step: 1025 Val Loss: 1.6351330280303955\n",
            "Epoch: 2 Step: 1050 Val Loss: 1.6271393299102783\n",
            "Epoch: 2 Step: 1075 Val Loss: 1.616065502166748\n",
            "Epoch: 2 Step: 1100 Val Loss: 1.6101230382919312\n",
            "Epoch: 2 Step: 1125 Val Loss: 1.5973031520843506\n",
            "Epoch: 2 Step: 1150 Val Loss: 1.595365047454834\n",
            "Epoch: 2 Step: 1175 Val Loss: 1.5816459655761719\n",
            "Epoch: 2 Step: 1200 Val Loss: 1.5728564262390137\n",
            "Epoch: 2 Step: 1225 Val Loss: 1.5709526538848877\n",
            "Epoch: 2 Step: 1250 Val Loss: 1.5621615648269653\n",
            "Epoch: 2 Step: 1275 Val Loss: 1.565789818763733\n",
            "Epoch: 2 Step: 1300 Val Loss: 1.5543490648269653\n",
            "Epoch: 2 Step: 1325 Val Loss: 1.5501482486724854\n",
            "Epoch: 2 Step: 1350 Val Loss: 1.5399774312973022\n",
            "Epoch: 2 Step: 1375 Val Loss: 1.5424060821533203\n",
            "Epoch: 2 Step: 1400 Val Loss: 1.5312672853469849\n",
            "Epoch: 2 Step: 1425 Val Loss: 1.5266883373260498\n",
            "Epoch: 2 Step: 1450 Val Loss: 1.5173256397247314\n",
            "Epoch: 3 Step: 1475 Val Loss: 1.5171986818313599\n",
            "Epoch: 3 Step: 1500 Val Loss: 1.5090407133102417\n",
            "Epoch: 3 Step: 1525 Val Loss: 1.5097908973693848\n",
            "Epoch: 3 Step: 1550 Val Loss: 1.5066817998886108\n",
            "Epoch: 3 Step: 1575 Val Loss: 1.5042548179626465\n",
            "Epoch: 3 Step: 1600 Val Loss: 1.4905705451965332\n",
            "Epoch: 3 Step: 1625 Val Loss: 1.4892085790634155\n",
            "Epoch: 3 Step: 1650 Val Loss: 1.4881824254989624\n",
            "Epoch: 3 Step: 1675 Val Loss: 1.482611060142517\n",
            "Epoch: 3 Step: 1700 Val Loss: 1.4762587547302246\n",
            "Epoch: 3 Step: 1725 Val Loss: 1.4799683094024658\n",
            "Epoch: 3 Step: 1750 Val Loss: 1.4759256839752197\n",
            "Epoch: 3 Step: 1775 Val Loss: 1.46965491771698\n",
            "Epoch: 3 Step: 1800 Val Loss: 1.4675743579864502\n",
            "Epoch: 3 Step: 1825 Val Loss: 1.4655097723007202\n",
            "Epoch: 3 Step: 1850 Val Loss: 1.4648200273513794\n",
            "Epoch: 3 Step: 1875 Val Loss: 1.4643356800079346\n",
            "Epoch: 3 Step: 1900 Val Loss: 1.456316590309143\n",
            "Epoch: 3 Step: 1925 Val Loss: 1.45556640625\n",
            "Epoch: 3 Step: 1950 Val Loss: 1.4459717273712158\n",
            "Epoch: 4 Step: 1975 Val Loss: 1.4478645324707031\n",
            "Epoch: 4 Step: 2000 Val Loss: 1.4508877992630005\n",
            "Epoch: 4 Step: 2025 Val Loss: 1.4417060613632202\n",
            "Epoch: 4 Step: 2050 Val Loss: 1.4409300088882446\n",
            "Epoch: 4 Step: 2075 Val Loss: 1.4426299333572388\n",
            "Epoch: 4 Step: 2100 Val Loss: 1.433850884437561\n",
            "Epoch: 4 Step: 2125 Val Loss: 1.4360089302062988\n",
            "Epoch: 4 Step: 2150 Val Loss: 1.4277710914611816\n",
            "Epoch: 4 Step: 2175 Val Loss: 1.4262841939926147\n",
            "Epoch: 4 Step: 2200 Val Loss: 1.426592230796814\n",
            "Epoch: 4 Step: 2225 Val Loss: 1.4238675832748413\n",
            "Epoch: 4 Step: 2250 Val Loss: 1.4158117771148682\n",
            "Epoch: 4 Step: 2275 Val Loss: 1.421207308769226\n",
            "Epoch: 4 Step: 2300 Val Loss: 1.415710210800171\n",
            "Epoch: 4 Step: 2325 Val Loss: 1.419308066368103\n",
            "Epoch: 4 Step: 2350 Val Loss: 1.4129587411880493\n",
            "Epoch: 4 Step: 2375 Val Loss: 1.4149802923202515\n",
            "Epoch: 4 Step: 2400 Val Loss: 1.4141075611114502\n",
            "Epoch: 4 Step: 2425 Val Loss: 1.4102193117141724\n",
            "Epoch: 4 Step: 2450 Val Loss: 1.4072561264038086\n",
            "Epoch: 5 Step: 2475 Val Loss: 1.406729817390442\n",
            "Epoch: 5 Step: 2500 Val Loss: 1.404848575592041\n",
            "Epoch: 5 Step: 2525 Val Loss: 1.4078385829925537\n",
            "Epoch: 5 Step: 2550 Val Loss: 1.4034991264343262\n",
            "Epoch: 5 Step: 2575 Val Loss: 1.3961431980133057\n",
            "Epoch: 5 Step: 2600 Val Loss: 1.399090051651001\n",
            "Epoch: 5 Step: 2625 Val Loss: 1.3952651023864746\n",
            "Epoch: 5 Step: 2650 Val Loss: 1.3963127136230469\n",
            "Epoch: 5 Step: 2675 Val Loss: 1.3905736207962036\n",
            "Epoch: 5 Step: 2700 Val Loss: 1.391575574874878\n",
            "Epoch: 5 Step: 2725 Val Loss: 1.392903447151184\n",
            "Epoch: 5 Step: 2750 Val Loss: 1.3900564908981323\n",
            "Epoch: 5 Step: 2775 Val Loss: 1.3866729736328125\n",
            "Epoch: 5 Step: 2800 Val Loss: 1.3870453834533691\n",
            "Epoch: 5 Step: 2825 Val Loss: 1.389386773109436\n",
            "Epoch: 5 Step: 2850 Val Loss: 1.3910913467407227\n",
            "Epoch: 5 Step: 2875 Val Loss: 1.3897578716278076\n",
            "Epoch: 5 Step: 2900 Val Loss: 1.3861297369003296\n",
            "Epoch: 5 Step: 2925 Val Loss: 1.3790653944015503\n",
            "Epoch: 6 Step: 2950 Val Loss: 1.3781689405441284\n",
            "Epoch: 6 Step: 2975 Val Loss: 1.382116436958313\n",
            "Epoch: 6 Step: 3000 Val Loss: 1.3758606910705566\n",
            "Epoch: 6 Step: 3025 Val Loss: 1.377485990524292\n",
            "Epoch: 6 Step: 3050 Val Loss: 1.3767292499542236\n",
            "Epoch: 6 Step: 3075 Val Loss: 1.3763033151626587\n",
            "Epoch: 6 Step: 3100 Val Loss: 1.3728301525115967\n",
            "Epoch: 6 Step: 3125 Val Loss: 1.3685376644134521\n",
            "Epoch: 6 Step: 3150 Val Loss: 1.3748139142990112\n",
            "Epoch: 6 Step: 3175 Val Loss: 1.3722634315490723\n",
            "Epoch: 6 Step: 3200 Val Loss: 1.3672540187835693\n",
            "Epoch: 6 Step: 3225 Val Loss: 1.36513090133667\n",
            "Epoch: 6 Step: 3250 Val Loss: 1.3671892881393433\n",
            "Epoch: 6 Step: 3275 Val Loss: 1.3655742406845093\n",
            "Epoch: 6 Step: 3300 Val Loss: 1.370942234992981\n",
            "Epoch: 6 Step: 3325 Val Loss: 1.3726180791854858\n",
            "Epoch: 6 Step: 3350 Val Loss: 1.3676079511642456\n",
            "Epoch: 6 Step: 3375 Val Loss: 1.3656867742538452\n",
            "Epoch: 6 Step: 3400 Val Loss: 1.3663235902786255\n",
            "Epoch: 6 Step: 3425 Val Loss: 1.3603650331497192\n",
            "Epoch: 7 Step: 3450 Val Loss: 1.366505742073059\n",
            "Epoch: 7 Step: 3475 Val Loss: 1.3630818128585815\n",
            "Epoch: 7 Step: 3500 Val Loss: 1.3633220195770264\n",
            "Epoch: 7 Step: 3525 Val Loss: 1.357313632965088\n",
            "Epoch: 7 Step: 3550 Val Loss: 1.3563213348388672\n",
            "Epoch: 7 Step: 3575 Val Loss: 1.3620227575302124\n",
            "Epoch: 7 Step: 3600 Val Loss: 1.3589195013046265\n",
            "Epoch: 7 Step: 3625 Val Loss: 1.3536994457244873\n",
            "Epoch: 7 Step: 3650 Val Loss: 1.3593099117279053\n",
            "Epoch: 7 Step: 3675 Val Loss: 1.3534024953842163\n",
            "Epoch: 7 Step: 3700 Val Loss: 1.3503692150115967\n",
            "Epoch: 7 Step: 3725 Val Loss: 1.3528412580490112\n",
            "Epoch: 7 Step: 3750 Val Loss: 1.3530480861663818\n",
            "Epoch: 7 Step: 3775 Val Loss: 1.3521877527236938\n",
            "Epoch: 7 Step: 3800 Val Loss: 1.354354739189148\n",
            "Epoch: 7 Step: 3825 Val Loss: 1.3536128997802734\n",
            "Epoch: 7 Step: 3850 Val Loss: 1.3569389581680298\n",
            "Epoch: 7 Step: 3875 Val Loss: 1.3537147045135498\n",
            "Epoch: 7 Step: 3900 Val Loss: 1.348784327507019\n",
            "Epoch: 8 Step: 3925 Val Loss: 1.355514407157898\n",
            "Epoch: 8 Step: 3950 Val Loss: 1.349764108657837\n",
            "Epoch: 8 Step: 3975 Val Loss: 1.3524973392486572\n",
            "Epoch: 8 Step: 4000 Val Loss: 1.3518097400665283\n",
            "Epoch: 8 Step: 4025 Val Loss: 1.3512786626815796\n",
            "Epoch: 8 Step: 4050 Val Loss: 1.350525975227356\n",
            "Epoch: 8 Step: 4075 Val Loss: 1.3505033254623413\n",
            "Epoch: 8 Step: 4100 Val Loss: 1.34463369846344\n",
            "Epoch: 8 Step: 4125 Val Loss: 1.3458994626998901\n",
            "Epoch: 8 Step: 4150 Val Loss: 1.3375458717346191\n",
            "Epoch: 8 Step: 4175 Val Loss: 1.3427895307540894\n",
            "Epoch: 8 Step: 4200 Val Loss: 1.3397395610809326\n",
            "Epoch: 8 Step: 4225 Val Loss: 1.3399162292480469\n",
            "Epoch: 8 Step: 4250 Val Loss: 1.339577078819275\n",
            "Epoch: 8 Step: 4275 Val Loss: 1.342149257659912\n",
            "Epoch: 8 Step: 4300 Val Loss: 1.3436830043792725\n",
            "Epoch: 8 Step: 4325 Val Loss: 1.3457621335983276\n",
            "Epoch: 8 Step: 4350 Val Loss: 1.3426328897476196\n",
            "Epoch: 8 Step: 4375 Val Loss: 1.3459210395812988\n",
            "Epoch: 8 Step: 4400 Val Loss: 1.3459707498550415\n",
            "Epoch: 9 Step: 4425 Val Loss: 1.3393572568893433\n",
            "Epoch: 9 Step: 4450 Val Loss: 1.3404825925827026\n",
            "Epoch: 9 Step: 4475 Val Loss: 1.340467929840088\n",
            "Epoch: 9 Step: 4500 Val Loss: 1.3366749286651611\n",
            "Epoch: 9 Step: 4525 Val Loss: 1.339874029159546\n",
            "Epoch: 9 Step: 4550 Val Loss: 1.3381587266921997\n",
            "Epoch: 9 Step: 4575 Val Loss: 1.3388714790344238\n",
            "Epoch: 9 Step: 4600 Val Loss: 1.3335354328155518\n",
            "Epoch: 9 Step: 4625 Val Loss: 1.3367745876312256\n",
            "Epoch: 9 Step: 4650 Val Loss: 1.3350657224655151\n",
            "Epoch: 9 Step: 4675 Val Loss: 1.3305675983428955\n",
            "Epoch: 9 Step: 4700 Val Loss: 1.3346550464630127\n",
            "Epoch: 9 Step: 4725 Val Loss: 1.3339011669158936\n",
            "Epoch: 9 Step: 4750 Val Loss: 1.3325581550598145\n",
            "Epoch: 9 Step: 4775 Val Loss: 1.3367403745651245\n",
            "Epoch: 9 Step: 4800 Val Loss: 1.3363406658172607\n",
            "Epoch: 9 Step: 4825 Val Loss: 1.339902400970459\n",
            "Epoch: 9 Step: 4850 Val Loss: 1.3370071649551392\n",
            "Epoch: 9 Step: 4875 Val Loss: 1.3347607851028442\n",
            "Epoch: 9 Step: 4900 Val Loss: 1.3307241201400757\n",
            "Epoch: 10 Step: 4925 Val Loss: 1.3309942483901978\n",
            "Epoch: 10 Step: 4950 Val Loss: 1.334110140800476\n",
            "Epoch: 10 Step: 4975 Val Loss: 1.3341169357299805\n",
            "Epoch: 10 Step: 5000 Val Loss: 1.3367133140563965\n",
            "Epoch: 10 Step: 5025 Val Loss: 1.331973910331726\n",
            "Epoch: 10 Step: 5050 Val Loss: 1.3286242485046387\n",
            "Epoch: 10 Step: 5075 Val Loss: 1.3302501440048218\n",
            "Epoch: 10 Step: 5100 Val Loss: 1.3323107957839966\n",
            "Epoch: 10 Step: 5125 Val Loss: 1.332883358001709\n",
            "Epoch: 10 Step: 5150 Val Loss: 1.3312346935272217\n",
            "Epoch: 10 Step: 5175 Val Loss: 1.3298711776733398\n",
            "Epoch: 10 Step: 5200 Val Loss: 1.3294687271118164\n",
            "Epoch: 10 Step: 5225 Val Loss: 1.3278735876083374\n",
            "Epoch: 10 Step: 5250 Val Loss: 1.3308460712432861\n",
            "Epoch: 10 Step: 5275 Val Loss: 1.3255261182785034\n",
            "Epoch: 10 Step: 5300 Val Loss: 1.3318026065826416\n",
            "Epoch: 10 Step: 5325 Val Loss: 1.3282142877578735\n",
            "Epoch: 10 Step: 5350 Val Loss: 1.328551173210144\n",
            "Epoch: 10 Step: 5375 Val Loss: 1.3261953592300415\n",
            "Epoch: 11 Step: 5400 Val Loss: 1.3263001441955566\n",
            "Epoch: 11 Step: 5425 Val Loss: 1.325724482536316\n",
            "Epoch: 11 Step: 5450 Val Loss: 1.333492636680603\n",
            "Epoch: 11 Step: 5475 Val Loss: 1.3282133340835571\n",
            "Epoch: 11 Step: 5500 Val Loss: 1.327235460281372\n",
            "Epoch: 11 Step: 5525 Val Loss: 1.3258925676345825\n",
            "Epoch: 11 Step: 5550 Val Loss: 1.3225445747375488\n",
            "Epoch: 11 Step: 5575 Val Loss: 1.3155399560928345\n",
            "Epoch: 11 Step: 5600 Val Loss: 1.3239467144012451\n",
            "Epoch: 11 Step: 5625 Val Loss: 1.3236377239227295\n",
            "Epoch: 11 Step: 5650 Val Loss: 1.3207857608795166\n",
            "Epoch: 11 Step: 5675 Val Loss: 1.3224738836288452\n",
            "Epoch: 11 Step: 5700 Val Loss: 1.3235769271850586\n",
            "Epoch: 11 Step: 5725 Val Loss: 1.3239573240280151\n",
            "Epoch: 11 Step: 5750 Val Loss: 1.3274922370910645\n",
            "Epoch: 11 Step: 5775 Val Loss: 1.329040288925171\n",
            "Epoch: 11 Step: 5800 Val Loss: 1.3269037008285522\n",
            "Epoch: 11 Step: 5825 Val Loss: 1.3260067701339722\n",
            "Epoch: 11 Step: 5850 Val Loss: 1.325811743736267\n",
            "Epoch: 11 Step: 5875 Val Loss: 1.322654128074646\n",
            "Epoch: 12 Step: 5900 Val Loss: 1.3216745853424072\n",
            "Epoch: 12 Step: 5925 Val Loss: 1.3287506103515625\n",
            "Epoch: 12 Step: 5950 Val Loss: 1.3267217874526978\n",
            "Epoch: 12 Step: 5975 Val Loss: 1.3215913772583008\n",
            "Epoch: 12 Step: 6000 Val Loss: 1.319247841835022\n",
            "Epoch: 12 Step: 6025 Val Loss: 1.321138620376587\n",
            "Epoch: 12 Step: 6050 Val Loss: 1.3228799104690552\n",
            "Epoch: 12 Step: 6075 Val Loss: 1.3189510107040405\n",
            "Epoch: 12 Step: 6100 Val Loss: 1.32000732421875\n",
            "Epoch: 12 Step: 6125 Val Loss: 1.3185303211212158\n",
            "Epoch: 12 Step: 6150 Val Loss: 1.3184460401535034\n",
            "Epoch: 12 Step: 6175 Val Loss: 1.3196505308151245\n",
            "Epoch: 12 Step: 6200 Val Loss: 1.316787838935852\n",
            "Epoch: 12 Step: 6225 Val Loss: 1.321596384048462\n",
            "Epoch: 12 Step: 6250 Val Loss: 1.3204866647720337\n",
            "Epoch: 12 Step: 6275 Val Loss: 1.3228908777236938\n",
            "Epoch: 12 Step: 6300 Val Loss: 1.325135588645935\n",
            "Epoch: 12 Step: 6325 Val Loss: 1.3187583684921265\n",
            "Epoch: 12 Step: 6350 Val Loss: 1.3193989992141724\n",
            "Epoch: 13 Step: 6375 Val Loss: 1.3205626010894775\n",
            "Epoch: 13 Step: 6400 Val Loss: 1.3230324983596802\n",
            "Epoch: 13 Step: 6425 Val Loss: 1.3186562061309814\n",
            "Epoch: 13 Step: 6450 Val Loss: 1.3164664506912231\n",
            "Epoch: 13 Step: 6475 Val Loss: 1.3191981315612793\n",
            "Epoch: 13 Step: 6500 Val Loss: 1.3158667087554932\n",
            "Epoch: 13 Step: 6525 Val Loss: 1.3217716217041016\n",
            "Epoch: 13 Step: 6550 Val Loss: 1.3163044452667236\n",
            "Epoch: 13 Step: 6575 Val Loss: 1.3164554834365845\n",
            "Epoch: 13 Step: 6600 Val Loss: 1.3153395652770996\n",
            "Epoch: 13 Step: 6625 Val Loss: 1.3124178647994995\n",
            "Epoch: 13 Step: 6650 Val Loss: 1.3146013021469116\n",
            "Epoch: 13 Step: 6675 Val Loss: 1.312585473060608\n",
            "Epoch: 13 Step: 6700 Val Loss: 1.311950445175171\n",
            "Epoch: 13 Step: 6725 Val Loss: 1.32099187374115\n",
            "Epoch: 13 Step: 6750 Val Loss: 1.314401388168335\n",
            "Epoch: 13 Step: 6775 Val Loss: 1.3191672563552856\n",
            "Epoch: 13 Step: 6800 Val Loss: 1.3161909580230713\n",
            "Epoch: 13 Step: 6825 Val Loss: 1.316675066947937\n",
            "Epoch: 13 Step: 6850 Val Loss: 1.3157254457473755\n",
            "Epoch: 14 Step: 6875 Val Loss: 1.3140645027160645\n",
            "Epoch: 14 Step: 6900 Val Loss: 1.3147332668304443\n",
            "Epoch: 14 Step: 6925 Val Loss: 1.3137519359588623\n",
            "Epoch: 14 Step: 6950 Val Loss: 1.3186302185058594\n",
            "Epoch: 14 Step: 6975 Val Loss: 1.3209737539291382\n",
            "Epoch: 14 Step: 7000 Val Loss: 1.3151663541793823\n",
            "Epoch: 14 Step: 7025 Val Loss: 1.311574935913086\n",
            "Epoch: 14 Step: 7050 Val Loss: 1.3162734508514404\n",
            "Epoch: 14 Step: 7075 Val Loss: 1.311627984046936\n",
            "Epoch: 14 Step: 7100 Val Loss: 1.312317967414856\n",
            "Epoch: 14 Step: 7125 Val Loss: 1.3091297149658203\n",
            "Epoch: 14 Step: 7150 Val Loss: 1.3163306713104248\n",
            "Epoch: 14 Step: 7175 Val Loss: 1.3064621686935425\n",
            "Epoch: 14 Step: 7200 Val Loss: 1.311596155166626\n",
            "Epoch: 14 Step: 7225 Val Loss: 1.3151142597198486\n",
            "Epoch: 14 Step: 7250 Val Loss: 1.3165007829666138\n",
            "Epoch: 14 Step: 7275 Val Loss: 1.3165041208267212\n",
            "Epoch: 14 Step: 7300 Val Loss: 1.3132872581481934\n",
            "Epoch: 14 Step: 7325 Val Loss: 1.3156179189682007\n",
            "Epoch: 14 Step: 7350 Val Loss: 1.3155537843704224\n",
            "Epoch: 15 Step: 7375 Val Loss: 1.31610906124115\n",
            "Epoch: 15 Step: 7400 Val Loss: 1.3174960613250732\n",
            "Epoch: 15 Step: 7425 Val Loss: 1.3175419569015503\n",
            "Epoch: 15 Step: 7450 Val Loss: 1.3197708129882812\n",
            "Epoch: 15 Step: 7475 Val Loss: 1.3106118440628052\n",
            "Epoch: 15 Step: 7500 Val Loss: 1.3123899698257446\n",
            "Epoch: 15 Step: 7525 Val Loss: 1.3162068128585815\n",
            "Epoch: 15 Step: 7550 Val Loss: 1.313856601715088\n",
            "Epoch: 15 Step: 7575 Val Loss: 1.3136169910430908\n",
            "Epoch: 15 Step: 7600 Val Loss: 1.3125735521316528\n",
            "Epoch: 15 Step: 7625 Val Loss: 1.3128209114074707\n",
            "Epoch: 15 Step: 7650 Val Loss: 1.3127665519714355\n",
            "Epoch: 15 Step: 7675 Val Loss: 1.3063843250274658\n",
            "Epoch: 15 Step: 7700 Val Loss: 1.315224051475525\n",
            "Epoch: 15 Step: 7725 Val Loss: 1.3117601871490479\n",
            "Epoch: 15 Step: 7750 Val Loss: 1.3098180294036865\n",
            "Epoch: 15 Step: 7775 Val Loss: 1.3107879161834717\n",
            "Epoch: 15 Step: 7800 Val Loss: 1.309053897857666\n",
            "Epoch: 15 Step: 7825 Val Loss: 1.3099286556243896\n",
            "Epoch: 16 Step: 7850 Val Loss: 1.3113595247268677\n",
            "Epoch: 16 Step: 7875 Val Loss: 1.309275507926941\n",
            "Epoch: 16 Step: 7900 Val Loss: 1.3162208795547485\n",
            "Epoch: 16 Step: 7925 Val Loss: 1.3139451742172241\n",
            "Epoch: 16 Step: 7950 Val Loss: 1.315035104751587\n",
            "Epoch: 16 Step: 7975 Val Loss: 1.305747389793396\n",
            "Epoch: 16 Step: 8000 Val Loss: 1.3044517040252686\n",
            "Epoch: 16 Step: 8025 Val Loss: 1.3067409992218018\n",
            "Epoch: 16 Step: 8050 Val Loss: 1.3141189813613892\n",
            "Epoch: 16 Step: 8075 Val Loss: 1.3122155666351318\n",
            "Epoch: 16 Step: 8100 Val Loss: 1.3064948320388794\n",
            "Epoch: 16 Step: 8125 Val Loss: 1.3052741289138794\n",
            "Epoch: 16 Step: 8150 Val Loss: 1.3044908046722412\n",
            "Epoch: 16 Step: 8175 Val Loss: 1.3025169372558594\n",
            "Epoch: 16 Step: 8200 Val Loss: 1.3078497648239136\n",
            "Epoch: 16 Step: 8225 Val Loss: 1.308264136314392\n",
            "Epoch: 16 Step: 8250 Val Loss: 1.312984824180603\n",
            "Epoch: 16 Step: 8275 Val Loss: 1.30732262134552\n",
            "Epoch: 16 Step: 8300 Val Loss: 1.3062258958816528\n",
            "Epoch: 16 Step: 8325 Val Loss: 1.308916449546814\n",
            "Epoch: 17 Step: 8350 Val Loss: 1.3086122274398804\n",
            "Epoch: 17 Step: 8375 Val Loss: 1.3089640140533447\n",
            "Epoch: 17 Step: 8400 Val Loss: 1.317718744277954\n",
            "Epoch: 17 Step: 8425 Val Loss: 1.3086961507797241\n",
            "Epoch: 17 Step: 8450 Val Loss: 1.3072625398635864\n",
            "Epoch: 17 Step: 8475 Val Loss: 1.3070874214172363\n",
            "Epoch: 17 Step: 8500 Val Loss: 1.3092843294143677\n",
            "Epoch: 17 Step: 8525 Val Loss: 1.3024934530258179\n",
            "Epoch: 17 Step: 8550 Val Loss: 1.311141014099121\n",
            "Epoch: 17 Step: 8575 Val Loss: 1.3124463558197021\n",
            "Epoch: 17 Step: 8600 Val Loss: 1.3038206100463867\n",
            "Epoch: 17 Step: 8625 Val Loss: 1.308670163154602\n",
            "Epoch: 17 Step: 8650 Val Loss: 1.3049978017807007\n",
            "Epoch: 17 Step: 8675 Val Loss: 1.306792974472046\n",
            "Epoch: 17 Step: 8700 Val Loss: 1.3105400800704956\n",
            "Epoch: 17 Step: 8725 Val Loss: 1.3152742385864258\n",
            "Epoch: 17 Step: 8750 Val Loss: 1.3145618438720703\n",
            "Epoch: 17 Step: 8775 Val Loss: 1.3128466606140137\n",
            "Epoch: 17 Step: 8800 Val Loss: 1.3079391717910767\n",
            "Epoch: 18 Step: 8825 Val Loss: 1.3098325729370117\n",
            "Epoch: 18 Step: 8850 Val Loss: 1.3073943853378296\n",
            "Epoch: 18 Step: 8875 Val Loss: 1.3116434812545776\n",
            "Epoch: 18 Step: 8900 Val Loss: 1.3106520175933838\n",
            "Epoch: 18 Step: 8925 Val Loss: 1.3131344318389893\n",
            "Epoch: 18 Step: 8950 Val Loss: 1.304126262664795\n",
            "Epoch: 18 Step: 8975 Val Loss: 1.3107202053070068\n",
            "Epoch: 18 Step: 9000 Val Loss: 1.3080968856811523\n",
            "Epoch: 18 Step: 9025 Val Loss: 1.308068037033081\n",
            "Epoch: 18 Step: 9050 Val Loss: 1.3103885650634766\n",
            "Epoch: 18 Step: 9075 Val Loss: 1.3077868223190308\n",
            "Epoch: 18 Step: 9100 Val Loss: 1.3073179721832275\n",
            "Epoch: 18 Step: 9125 Val Loss: 1.3029799461364746\n",
            "Epoch: 18 Step: 9150 Val Loss: 1.2999557256698608\n",
            "Epoch: 18 Step: 9175 Val Loss: 1.3093340396881104\n",
            "Epoch: 18 Step: 9200 Val Loss: 1.3058243989944458\n",
            "Epoch: 18 Step: 9225 Val Loss: 1.310502290725708\n",
            "Epoch: 18 Step: 9250 Val Loss: 1.3109104633331299\n",
            "Epoch: 18 Step: 9275 Val Loss: 1.306164026260376\n",
            "Epoch: 18 Step: 9300 Val Loss: 1.3037656545639038\n",
            "Epoch: 19 Step: 9325 Val Loss: 1.3010473251342773\n",
            "Epoch: 19 Step: 9350 Val Loss: 1.3025528192520142\n",
            "Epoch: 19 Step: 9375 Val Loss: 1.3047478199005127\n",
            "Epoch: 19 Step: 9400 Val Loss: 1.3058171272277832\n",
            "Epoch: 19 Step: 9425 Val Loss: 1.306096076965332\n",
            "Epoch: 19 Step: 9450 Val Loss: 1.3062877655029297\n",
            "Epoch: 19 Step: 9475 Val Loss: 1.3068344593048096\n",
            "Epoch: 19 Step: 9500 Val Loss: 1.300155758857727\n",
            "Epoch: 19 Step: 9525 Val Loss: 1.3032602071762085\n",
            "Epoch: 19 Step: 9550 Val Loss: 1.3048232793807983\n",
            "Epoch: 19 Step: 9575 Val Loss: 1.2980941534042358\n",
            "Epoch: 19 Step: 9600 Val Loss: 1.3020790815353394\n",
            "Epoch: 19 Step: 9625 Val Loss: 1.3000094890594482\n",
            "Epoch: 19 Step: 9650 Val Loss: 1.3021268844604492\n",
            "Epoch: 19 Step: 9675 Val Loss: 1.306087613105774\n",
            "Epoch: 19 Step: 9700 Val Loss: 1.3053982257843018\n",
            "Epoch: 19 Step: 9725 Val Loss: 1.3066295385360718\n",
            "Epoch: 19 Step: 9750 Val Loss: 1.3033429384231567\n",
            "Epoch: 19 Step: 9775 Val Loss: 1.3048763275146484\n",
            "Epoch: 19 Step: 9800 Val Loss: 1.3046958446502686\n",
            "Epoch: 20 Step: 9825 Val Loss: 1.307610273361206\n",
            "Epoch: 20 Step: 9850 Val Loss: 1.3113517761230469\n",
            "Epoch: 20 Step: 9875 Val Loss: 1.3098104000091553\n",
            "Epoch: 20 Step: 9900 Val Loss: 1.310530185699463\n",
            "Epoch: 20 Step: 9925 Val Loss: 1.303488850593567\n",
            "Epoch: 20 Step: 9950 Val Loss: 1.3057572841644287\n",
            "Epoch: 20 Step: 9975 Val Loss: 1.3066755533218384\n",
            "Epoch: 20 Step: 10000 Val Loss: 1.304813265800476\n",
            "Epoch: 20 Step: 10025 Val Loss: 1.3080766201019287\n",
            "Epoch: 20 Step: 10050 Val Loss: 1.301649808883667\n",
            "Epoch: 20 Step: 10075 Val Loss: 1.310105800628662\n",
            "Epoch: 20 Step: 10100 Val Loss: 1.3025165796279907\n",
            "Epoch: 20 Step: 10125 Val Loss: 1.2980111837387085\n",
            "Epoch: 20 Step: 10150 Val Loss: 1.308034062385559\n",
            "Epoch: 20 Step: 10175 Val Loss: 1.3023178577423096\n",
            "Epoch: 20 Step: 10200 Val Loss: 1.3087810277938843\n",
            "Epoch: 20 Step: 10225 Val Loss: 1.3097285032272339\n",
            "Epoch: 20 Step: 10250 Val Loss: 1.3078796863555908\n",
            "Epoch: 20 Step: 10275 Val Loss: 1.305271863937378\n",
            "Epoch: 21 Step: 10300 Val Loss: 1.3067415952682495\n",
            "Epoch: 21 Step: 10325 Val Loss: 1.3103878498077393\n",
            "Epoch: 21 Step: 10350 Val Loss: 1.3095611333847046\n",
            "Epoch: 21 Step: 10375 Val Loss: 1.3081895112991333\n",
            "Epoch: 21 Step: 10400 Val Loss: 1.3108536005020142\n",
            "Epoch: 21 Step: 10425 Val Loss: 1.302030086517334\n",
            "Epoch: 21 Step: 10450 Val Loss: 1.307070255279541\n",
            "Epoch: 21 Step: 10475 Val Loss: 1.305190086364746\n",
            "Epoch: 21 Step: 10500 Val Loss: 1.3045841455459595\n",
            "Epoch: 21 Step: 10525 Val Loss: 1.3050119876861572\n",
            "Epoch: 21 Step: 10550 Val Loss: 1.2999351024627686\n",
            "Epoch: 21 Step: 10575 Val Loss: 1.3036084175109863\n",
            "Epoch: 21 Step: 10600 Val Loss: 1.298921823501587\n",
            "Epoch: 21 Step: 10625 Val Loss: 1.2976535558700562\n",
            "Epoch: 21 Step: 10650 Val Loss: 1.2997612953186035\n",
            "Epoch: 21 Step: 10675 Val Loss: 1.3022547960281372\n",
            "Epoch: 21 Step: 10700 Val Loss: 1.3100813627243042\n",
            "Epoch: 21 Step: 10725 Val Loss: 1.3010692596435547\n",
            "Epoch: 21 Step: 10750 Val Loss: 1.3048505783081055\n",
            "Epoch: 21 Step: 10775 Val Loss: 1.302406668663025\n",
            "Epoch: 22 Step: 10800 Val Loss: 1.3026134967803955\n",
            "Epoch: 22 Step: 10825 Val Loss: 1.3045576810836792\n",
            "Epoch: 22 Step: 10850 Val Loss: 1.3121634721755981\n",
            "Epoch: 22 Step: 10875 Val Loss: 1.3000544309616089\n",
            "Epoch: 22 Step: 10900 Val Loss: 1.3088496923446655\n",
            "Epoch: 22 Step: 10925 Val Loss: 1.3047428131103516\n",
            "Epoch: 22 Step: 10950 Val Loss: 1.3046815395355225\n",
            "Epoch: 22 Step: 10975 Val Loss: 1.2978652715682983\n",
            "Epoch: 22 Step: 11000 Val Loss: 1.3062150478363037\n",
            "Epoch: 22 Step: 11025 Val Loss: 1.3025802373886108\n",
            "Epoch: 22 Step: 11050 Val Loss: 1.3007328510284424\n",
            "Epoch: 22 Step: 11075 Val Loss: 1.3024178743362427\n",
            "Epoch: 22 Step: 11100 Val Loss: 1.2948246002197266\n",
            "Epoch: 22 Step: 11125 Val Loss: 1.29668128490448\n",
            "Epoch: 22 Step: 11150 Val Loss: 1.3018825054168701\n",
            "Epoch: 22 Step: 11175 Val Loss: 1.302489995956421\n",
            "Epoch: 22 Step: 11200 Val Loss: 1.3076159954071045\n",
            "Epoch: 22 Step: 11225 Val Loss: 1.2980186939239502\n",
            "Epoch: 22 Step: 11250 Val Loss: 1.2957289218902588\n",
            "Epoch: 23 Step: 11275 Val Loss: 1.303846836090088\n",
            "Epoch: 23 Step: 11300 Val Loss: 1.2998590469360352\n",
            "Epoch: 23 Step: 11325 Val Loss: 1.3019307851791382\n",
            "Epoch: 23 Step: 11350 Val Loss: 1.3049657344818115\n",
            "Epoch: 23 Step: 11375 Val Loss: 1.3033714294433594\n",
            "Epoch: 23 Step: 11400 Val Loss: 1.301331877708435\n",
            "Epoch: 23 Step: 11425 Val Loss: 1.2998172044754028\n",
            "Epoch: 23 Step: 11450 Val Loss: 1.3000693321228027\n",
            "Epoch: 23 Step: 11475 Val Loss: 1.296905279159546\n",
            "Epoch: 23 Step: 11500 Val Loss: 1.3019064664840698\n",
            "Epoch: 23 Step: 11525 Val Loss: 1.296037197113037\n",
            "Epoch: 23 Step: 11550 Val Loss: 1.302223563194275\n",
            "Epoch: 23 Step: 11575 Val Loss: 1.2963916063308716\n",
            "Epoch: 23 Step: 11600 Val Loss: 1.2947406768798828\n",
            "Epoch: 23 Step: 11625 Val Loss: 1.3027193546295166\n",
            "Epoch: 23 Step: 11650 Val Loss: 1.3010027408599854\n",
            "Epoch: 23 Step: 11675 Val Loss: 1.3020896911621094\n",
            "Epoch: 23 Step: 11700 Val Loss: 1.3076355457305908\n",
            "Epoch: 23 Step: 11725 Val Loss: 1.3040261268615723\n",
            "Epoch: 23 Step: 11750 Val Loss: 1.2997961044311523\n",
            "Epoch: 24 Step: 11775 Val Loss: 1.3016248941421509\n",
            "Epoch: 24 Step: 11800 Val Loss: 1.3037960529327393\n",
            "Epoch: 24 Step: 11825 Val Loss: 1.3014910221099854\n",
            "Epoch: 24 Step: 11850 Val Loss: 1.3031281232833862\n",
            "Epoch: 24 Step: 11875 Val Loss: 1.302140474319458\n",
            "Epoch: 24 Step: 11900 Val Loss: 1.3037593364715576\n",
            "Epoch: 24 Step: 11925 Val Loss: 1.3007912635803223\n",
            "Epoch: 24 Step: 11950 Val Loss: 1.2978880405426025\n",
            "Epoch: 24 Step: 11975 Val Loss: 1.299294352531433\n",
            "Epoch: 24 Step: 12000 Val Loss: 1.3006346225738525\n",
            "Epoch: 24 Step: 12025 Val Loss: 1.2968950271606445\n",
            "Epoch: 24 Step: 12050 Val Loss: 1.2984049320220947\n",
            "Epoch: 24 Step: 12075 Val Loss: 1.2925435304641724\n",
            "Epoch: 24 Step: 12100 Val Loss: 1.296047568321228\n",
            "Epoch: 24 Step: 12125 Val Loss: 1.3026666641235352\n",
            "Epoch: 24 Step: 12150 Val Loss: 1.3039464950561523\n",
            "Epoch: 24 Step: 12175 Val Loss: 1.3045623302459717\n",
            "Epoch: 24 Step: 12200 Val Loss: 1.3003461360931396\n",
            "Epoch: 24 Step: 12225 Val Loss: 1.3016709089279175\n",
            "Epoch: 24 Step: 12250 Val Loss: 1.3028374910354614\n",
            "Epoch: 25 Step: 12275 Val Loss: 1.2997252941131592\n",
            "Epoch: 25 Step: 12300 Val Loss: 1.3041486740112305\n",
            "Epoch: 25 Step: 12325 Val Loss: 1.300531029701233\n",
            "Epoch: 25 Step: 12350 Val Loss: 1.304505705833435\n",
            "Epoch: 25 Step: 12375 Val Loss: 1.298410415649414\n",
            "Epoch: 25 Step: 12400 Val Loss: 1.295958161354065\n",
            "Epoch: 25 Step: 12425 Val Loss: 1.2962712049484253\n",
            "Epoch: 25 Step: 12450 Val Loss: 1.308234453201294\n",
            "Epoch: 25 Step: 12475 Val Loss: 1.3031646013259888\n",
            "Epoch: 25 Step: 12525 Val Loss: 1.304325819015503\n",
            "Epoch: 25 Step: 12550 Val Loss: 1.3048027753829956\n",
            "Epoch: 25 Step: 12575 Val Loss: 1.3005574941635132\n",
            "Epoch: 25 Step: 12600 Val Loss: 1.303987741470337\n",
            "Epoch: 25 Step: 12625 Val Loss: 1.3033608198165894\n",
            "Epoch: 25 Step: 12650 Val Loss: 1.3089611530303955\n",
            "Epoch: 25 Step: 12675 Val Loss: 1.3107274770736694\n",
            "Epoch: 25 Step: 12700 Val Loss: 1.3060705661773682\n",
            "Epoch: 25 Step: 12725 Val Loss: 1.303648591041565\n",
            "Epoch: 26 Step: 12750 Val Loss: 1.3030887842178345\n",
            "Epoch: 26 Step: 12775 Val Loss: 1.304856300354004\n",
            "Epoch: 26 Step: 12800 Val Loss: 1.3061574697494507\n",
            "Epoch: 26 Step: 12825 Val Loss: 1.303467035293579\n",
            "Epoch: 26 Step: 12850 Val Loss: 1.3041467666625977\n",
            "Epoch: 26 Step: 12875 Val Loss: 1.301016926765442\n",
            "Epoch: 26 Step: 12900 Val Loss: 1.2984482049942017\n",
            "Epoch: 26 Step: 12925 Val Loss: 1.296749472618103\n",
            "Epoch: 26 Step: 12950 Val Loss: 1.295452356338501\n",
            "Epoch: 26 Step: 12975 Val Loss: 1.2950211763381958\n",
            "Epoch: 26 Step: 13000 Val Loss: 1.2960114479064941\n",
            "Epoch: 26 Step: 13025 Val Loss: 1.2973915338516235\n",
            "Epoch: 26 Step: 13050 Val Loss: 1.3007574081420898\n",
            "Epoch: 26 Step: 13075 Val Loss: 1.2986390590667725\n",
            "Epoch: 26 Step: 13100 Val Loss: 1.3015919923782349\n",
            "Epoch: 26 Step: 13125 Val Loss: 1.300796627998352\n",
            "Epoch: 26 Step: 13150 Val Loss: 1.3091464042663574\n",
            "Epoch: 26 Step: 13175 Val Loss: 1.3016605377197266\n",
            "Epoch: 26 Step: 13200 Val Loss: 1.3035707473754883\n",
            "Epoch: 26 Step: 13225 Val Loss: 1.303256869316101\n",
            "Epoch: 27 Step: 13250 Val Loss: 1.3024652004241943\n",
            "Epoch: 27 Step: 13275 Val Loss: 1.302253246307373\n",
            "Epoch: 27 Step: 13300 Val Loss: 1.3032466173171997\n",
            "Epoch: 27 Step: 13325 Val Loss: 1.2963604927062988\n",
            "Epoch: 27 Step: 13350 Val Loss: 1.2993847131729126\n",
            "Epoch: 27 Step: 13375 Val Loss: 1.2988765239715576\n",
            "Epoch: 27 Step: 13400 Val Loss: 1.3023897409439087\n",
            "Epoch: 27 Step: 13425 Val Loss: 1.3023860454559326\n",
            "Epoch: 27 Step: 13450 Val Loss: 1.3037432432174683\n",
            "Epoch: 27 Step: 13475 Val Loss: 1.2995154857635498\n",
            "Epoch: 27 Step: 13500 Val Loss: 1.300266146659851\n",
            "Epoch: 27 Step: 13525 Val Loss: 1.296678900718689\n",
            "Epoch: 27 Step: 13550 Val Loss: 1.293735146522522\n",
            "Epoch: 27 Step: 13575 Val Loss: 1.2942521572113037\n",
            "Epoch: 27 Step: 13600 Val Loss: 1.306594967842102\n",
            "Epoch: 27 Step: 13625 Val Loss: 1.302215814590454\n",
            "Epoch: 27 Step: 13650 Val Loss: 1.3075215816497803\n",
            "Epoch: 27 Step: 13675 Val Loss: 1.3048697710037231\n",
            "Epoch: 27 Step: 13700 Val Loss: 1.3023228645324707\n",
            "Epoch: 28 Step: 13725 Val Loss: 1.3004257678985596\n",
            "Epoch: 28 Step: 13750 Val Loss: 1.2986737489700317\n",
            "Epoch: 28 Step: 13775 Val Loss: 1.3057974576950073\n",
            "Epoch: 28 Step: 13800 Val Loss: 1.304951548576355\n",
            "Epoch: 28 Step: 13825 Val Loss: 1.303794264793396\n",
            "Epoch: 28 Step: 13850 Val Loss: 1.3035224676132202\n",
            "Epoch: 28 Step: 13875 Val Loss: 1.300107479095459\n",
            "Epoch: 28 Step: 13900 Val Loss: 1.2979605197906494\n",
            "Epoch: 28 Step: 13925 Val Loss: 1.302046298980713\n",
            "Epoch: 28 Step: 13950 Val Loss: 1.3000704050064087\n",
            "Epoch: 28 Step: 13975 Val Loss: 1.2968809604644775\n",
            "Epoch: 28 Step: 14000 Val Loss: 1.2973344326019287\n",
            "Epoch: 28 Step: 14025 Val Loss: 1.2965830564498901\n",
            "Epoch: 28 Step: 14050 Val Loss: 1.292946219444275\n",
            "Epoch: 28 Step: 14075 Val Loss: 1.2995316982269287\n",
            "Epoch: 28 Step: 14100 Val Loss: 1.301466464996338\n",
            "Epoch: 28 Step: 14125 Val Loss: 1.2976422309875488\n",
            "Epoch: 28 Step: 14150 Val Loss: 1.306445837020874\n",
            "Epoch: 28 Step: 14175 Val Loss: 1.3012949228286743\n",
            "Epoch: 28 Step: 14200 Val Loss: 1.2960224151611328\n",
            "Epoch: 29 Step: 14225 Val Loss: 1.2952027320861816\n",
            "Epoch: 29 Step: 14250 Val Loss: 1.2970006465911865\n",
            "Epoch: 29 Step: 14275 Val Loss: 1.296886682510376\n",
            "Epoch: 29 Step: 14300 Val Loss: 1.2969255447387695\n",
            "Epoch: 29 Step: 14325 Val Loss: 1.297428011894226\n",
            "Epoch: 29 Step: 14350 Val Loss: 1.2975081205368042\n",
            "Epoch: 29 Step: 14375 Val Loss: 1.2999532222747803\n",
            "Epoch: 29 Step: 14400 Val Loss: 1.2985751628875732\n",
            "Epoch: 29 Step: 14425 Val Loss: 1.296109914779663\n",
            "Epoch: 29 Step: 14450 Val Loss: 1.2987040281295776\n",
            "Epoch: 29 Step: 14475 Val Loss: 1.2907905578613281\n",
            "Epoch: 29 Step: 14500 Val Loss: 1.2902445793151855\n",
            "Epoch: 29 Step: 14525 Val Loss: 1.2916473150253296\n",
            "Epoch: 29 Step: 14550 Val Loss: 1.29172945022583\n",
            "Epoch: 29 Step: 14575 Val Loss: 1.2942838668823242\n",
            "Epoch: 29 Step: 14600 Val Loss: 1.2972103357315063\n",
            "Epoch: 29 Step: 14625 Val Loss: 1.2969756126403809\n",
            "Epoch: 29 Step: 14650 Val Loss: 1.2979198694229126\n",
            "Epoch: 29 Step: 14675 Val Loss: 1.303786277770996\n",
            "Epoch: 29 Step: 14700 Val Loss: 1.3002004623413086\n",
            "Epoch: 30 Step: 14725 Val Loss: 1.30117666721344\n",
            "Epoch: 30 Step: 14750 Val Loss: 1.3064378499984741\n",
            "Epoch: 30 Step: 14775 Val Loss: 1.310254693031311\n",
            "Epoch: 30 Step: 14800 Val Loss: 1.3017019033432007\n",
            "Epoch: 30 Step: 14825 Val Loss: 1.2988851070404053\n",
            "Epoch: 30 Step: 14850 Val Loss: 1.3031797409057617\n",
            "Epoch: 30 Step: 14875 Val Loss: 1.2998671531677246\n",
            "Epoch: 30 Step: 14900 Val Loss: 1.2979882955551147\n",
            "Epoch: 30 Step: 14925 Val Loss: 1.2945849895477295\n",
            "Epoch: 30 Step: 14950 Val Loss: 1.292251467704773\n",
            "Epoch: 30 Step: 14975 Val Loss: 1.2999218702316284\n",
            "Epoch: 30 Step: 15000 Val Loss: 1.296090006828308\n",
            "Epoch: 30 Step: 15025 Val Loss: 1.290043830871582\n",
            "Epoch: 30 Step: 15050 Val Loss: 1.2999855279922485\n",
            "Epoch: 30 Step: 15075 Val Loss: 1.2986488342285156\n",
            "Epoch: 30 Step: 15100 Val Loss: 1.3019604682922363\n",
            "Epoch: 30 Step: 15125 Val Loss: 1.2965692281723022\n",
            "Epoch: 30 Step: 15150 Val Loss: 1.2990788221359253\n",
            "Epoch: 30 Step: 15175 Val Loss: 1.2968331575393677\n",
            "Epoch: 31 Step: 15200 Val Loss: 1.2986528873443604\n",
            "Epoch: 31 Step: 15225 Val Loss: 1.3001216650009155\n",
            "Epoch: 31 Step: 15250 Val Loss: 1.302917718887329\n",
            "Epoch: 31 Step: 15275 Val Loss: 1.2999032735824585\n",
            "Epoch: 31 Step: 15300 Val Loss: 1.2988661527633667\n",
            "Epoch: 31 Step: 15325 Val Loss: 1.2950564622879028\n",
            "Epoch: 31 Step: 15350 Val Loss: 1.2977410554885864\n",
            "Epoch: 31 Step: 15375 Val Loss: 1.292495608329773\n",
            "Epoch: 31 Step: 15400 Val Loss: 1.2958027124404907\n",
            "Epoch: 31 Step: 15425 Val Loss: 1.2974244356155396\n",
            "Epoch: 31 Step: 15450 Val Loss: 1.2920162677764893\n",
            "Epoch: 31 Step: 15475 Val Loss: 1.2907816171646118\n",
            "Epoch: 31 Step: 15500 Val Loss: 1.291507363319397\n",
            "Epoch: 31 Step: 15525 Val Loss: 1.2904536724090576\n",
            "Epoch: 31 Step: 15550 Val Loss: 1.296485424041748\n",
            "Epoch: 31 Step: 15575 Val Loss: 1.2963513135910034\n",
            "Epoch: 31 Step: 15600 Val Loss: 1.3028367757797241\n",
            "Epoch: 31 Step: 15625 Val Loss: 1.301369309425354\n",
            "Epoch: 31 Step: 15650 Val Loss: 1.3010121583938599\n",
            "Epoch: 31 Step: 15675 Val Loss: 1.2915711402893066\n",
            "Epoch: 32 Step: 15700 Val Loss: 1.294507384300232\n",
            "Epoch: 32 Step: 15725 Val Loss: 1.2950572967529297\n",
            "Epoch: 32 Step: 15750 Val Loss: 1.2986057996749878\n",
            "Epoch: 32 Step: 15775 Val Loss: 1.2914081811904907\n",
            "Epoch: 32 Step: 15800 Val Loss: 1.2950739860534668\n",
            "Epoch: 32 Step: 15825 Val Loss: 1.2934248447418213\n",
            "Epoch: 32 Step: 15850 Val Loss: 1.296388030052185\n",
            "Epoch: 32 Step: 15875 Val Loss: 1.2967547178268433\n",
            "Epoch: 32 Step: 15900 Val Loss: 1.2932077646255493\n",
            "Epoch: 32 Step: 15925 Val Loss: 1.291122317314148\n",
            "Epoch: 32 Step: 15950 Val Loss: 1.2914358377456665\n",
            "Epoch: 32 Step: 15975 Val Loss: 1.2869000434875488\n",
            "Epoch: 32 Step: 16000 Val Loss: 1.2872037887573242\n",
            "Epoch: 32 Step: 16025 Val Loss: 1.2917122840881348\n",
            "Epoch: 32 Step: 16050 Val Loss: 1.2940162420272827\n",
            "Epoch: 32 Step: 16075 Val Loss: 1.2965151071548462\n",
            "Epoch: 32 Step: 16100 Val Loss: 1.2982882261276245\n",
            "Epoch: 32 Step: 16125 Val Loss: 1.2969858646392822\n",
            "Epoch: 32 Step: 16150 Val Loss: 1.2927560806274414\n",
            "Epoch: 33 Step: 16175 Val Loss: 1.292418360710144\n",
            "Epoch: 33 Step: 16200 Val Loss: 1.2999281883239746\n",
            "Epoch: 33 Step: 16225 Val Loss: 1.298616647720337\n",
            "Epoch: 33 Step: 16250 Val Loss: 1.297750473022461\n",
            "Epoch: 33 Step: 16275 Val Loss: 1.2919803857803345\n",
            "Epoch: 33 Step: 16300 Val Loss: 1.2932538986206055\n",
            "Epoch: 33 Step: 16325 Val Loss: 1.2946358919143677\n",
            "Epoch: 33 Step: 16350 Val Loss: 1.2932456731796265\n",
            "Epoch: 33 Step: 16375 Val Loss: 1.2915005683898926\n",
            "Epoch: 33 Step: 16400 Val Loss: 1.2956583499908447\n",
            "Epoch: 33 Step: 16425 Val Loss: 1.291089415550232\n",
            "Epoch: 33 Step: 16450 Val Loss: 1.3003146648406982\n",
            "Epoch: 33 Step: 16475 Val Loss: 1.2938154935836792\n",
            "Epoch: 33 Step: 16500 Val Loss: 1.2939399480819702\n",
            "Epoch: 33 Step: 16525 Val Loss: 1.297929048538208\n",
            "Epoch: 33 Step: 16550 Val Loss: 1.2976598739624023\n",
            "Epoch: 33 Step: 16575 Val Loss: 1.2997491359710693\n",
            "Epoch: 33 Step: 16600 Val Loss: 1.3041729927062988\n",
            "Epoch: 33 Step: 16625 Val Loss: 1.3005930185317993\n",
            "Epoch: 33 Step: 16650 Val Loss: 1.2988862991333008\n",
            "Epoch: 34 Step: 16675 Val Loss: 1.289790153503418\n",
            "Epoch: 34 Step: 16700 Val Loss: 1.3112993240356445\n",
            "Epoch: 34 Step: 16725 Val Loss: 1.2978026866912842\n",
            "Epoch: 34 Step: 16750 Val Loss: 1.2957415580749512\n",
            "Epoch: 34 Step: 16775 Val Loss: 1.2962428331375122\n",
            "Epoch: 34 Step: 16800 Val Loss: 1.2935497760772705\n",
            "Epoch: 34 Step: 16825 Val Loss: 1.2985546588897705\n",
            "Epoch: 34 Step: 16850 Val Loss: 1.2923221588134766\n",
            "Epoch: 34 Step: 16875 Val Loss: 1.2883644104003906\n",
            "Epoch: 34 Step: 16900 Val Loss: 1.2925852537155151\n",
            "Epoch: 34 Step: 16925 Val Loss: 1.2900933027267456\n",
            "Epoch: 34 Step: 16950 Val Loss: 1.2919663190841675\n",
            "Epoch: 34 Step: 16975 Val Loss: 1.2921684980392456\n",
            "Epoch: 34 Step: 17000 Val Loss: 1.2882370948791504\n",
            "Epoch: 34 Step: 17025 Val Loss: 1.2955600023269653\n",
            "Epoch: 34 Step: 17050 Val Loss: 1.2954368591308594\n",
            "Epoch: 34 Step: 17075 Val Loss: 1.298488736152649\n",
            "Epoch: 34 Step: 17100 Val Loss: 1.299107313156128\n",
            "Epoch: 34 Step: 17125 Val Loss: 1.2991292476654053\n",
            "Epoch: 34 Step: 17150 Val Loss: 1.2936928272247314\n",
            "Epoch: 35 Step: 17175 Val Loss: 1.2945656776428223\n",
            "Epoch: 35 Step: 17200 Val Loss: 1.2969623804092407\n",
            "Epoch: 35 Step: 17225 Val Loss: 1.301987648010254\n",
            "Epoch: 35 Step: 17250 Val Loss: 1.298055648803711\n",
            "Epoch: 35 Step: 17275 Val Loss: 1.2980555295944214\n",
            "Epoch: 35 Step: 17300 Val Loss: 1.2956966161727905\n",
            "Epoch: 35 Step: 17325 Val Loss: 1.297714114189148\n",
            "Epoch: 35 Step: 17350 Val Loss: 1.2990374565124512\n",
            "Epoch: 35 Step: 17375 Val Loss: 1.2966647148132324\n",
            "Epoch: 35 Step: 17400 Val Loss: 1.295545220375061\n",
            "Epoch: 35 Step: 17425 Val Loss: 1.2962703704833984\n",
            "Epoch: 35 Step: 17450 Val Loss: 1.300180435180664\n",
            "Epoch: 35 Step: 17475 Val Loss: 1.2941408157348633\n",
            "Epoch: 35 Step: 17500 Val Loss: 1.2972279787063599\n",
            "Epoch: 35 Step: 17525 Val Loss: 1.298860788345337\n",
            "Epoch: 35 Step: 17550 Val Loss: 1.3025749921798706\n",
            "Epoch: 35 Step: 17575 Val Loss: 1.3001189231872559\n",
            "Epoch: 35 Step: 17600 Val Loss: 1.3002369403839111\n",
            "Epoch: 35 Step: 17625 Val Loss: 1.2985831499099731\n",
            "Epoch: 36 Step: 17650 Val Loss: 1.3009573221206665\n",
            "Epoch: 36 Step: 17675 Val Loss: 1.2999863624572754\n",
            "Epoch: 36 Step: 17700 Val Loss: 1.3044952154159546\n",
            "Epoch: 36 Step: 17725 Val Loss: 1.3031044006347656\n",
            "Epoch: 36 Step: 17750 Val Loss: 1.298989176750183\n",
            "Epoch: 36 Step: 17775 Val Loss: 1.2989753484725952\n",
            "Epoch: 36 Step: 17800 Val Loss: 1.3016841411590576\n",
            "Epoch: 36 Step: 17825 Val Loss: 1.3011040687561035\n",
            "Epoch: 36 Step: 17850 Val Loss: 1.3005201816558838\n",
            "Epoch: 36 Step: 17875 Val Loss: 1.3020247220993042\n",
            "Epoch: 36 Step: 17900 Val Loss: 1.2983099222183228\n",
            "Epoch: 36 Step: 17925 Val Loss: 1.3032931089401245\n",
            "Epoch: 36 Step: 17950 Val Loss: 1.2989369630813599\n",
            "Epoch: 36 Step: 17975 Val Loss: 1.2963805198669434\n",
            "Epoch: 36 Step: 18000 Val Loss: 1.301235318183899\n",
            "Epoch: 36 Step: 18025 Val Loss: 1.3017328977584839\n",
            "Epoch: 36 Step: 18050 Val Loss: 1.3015824556350708\n",
            "Epoch: 36 Step: 18075 Val Loss: 1.2988802194595337\n",
            "Epoch: 36 Step: 18100 Val Loss: 1.3003796339035034\n",
            "Epoch: 36 Step: 18125 Val Loss: 1.297311544418335\n",
            "Epoch: 37 Step: 18150 Val Loss: 1.3015450239181519\n",
            "Epoch: 37 Step: 18175 Val Loss: 1.3007845878601074\n",
            "Epoch: 37 Step: 18200 Val Loss: 1.2997547388076782\n",
            "Epoch: 37 Step: 18225 Val Loss: 1.2952373027801514\n",
            "Epoch: 37 Step: 18250 Val Loss: 1.2969368696212769\n",
            "Epoch: 37 Step: 18275 Val Loss: 1.2923516035079956\n",
            "Epoch: 37 Step: 18300 Val Loss: 1.2953952550888062\n",
            "Epoch: 37 Step: 18325 Val Loss: 1.2928221225738525\n",
            "Epoch: 37 Step: 18350 Val Loss: 1.2959831953048706\n",
            "Epoch: 37 Step: 18375 Val Loss: 1.2944979667663574\n",
            "Epoch: 37 Step: 18400 Val Loss: 1.2969319820404053\n",
            "Epoch: 37 Step: 18425 Val Loss: 1.2944016456604004\n",
            "Epoch: 37 Step: 18450 Val Loss: 1.2934437990188599\n",
            "Epoch: 37 Step: 18475 Val Loss: 1.29298734664917\n",
            "Epoch: 37 Step: 18500 Val Loss: 1.3001512289047241\n",
            "Epoch: 37 Step: 18525 Val Loss: 1.2986197471618652\n",
            "Epoch: 37 Step: 18550 Val Loss: 1.301802635192871\n",
            "Epoch: 37 Step: 18575 Val Loss: 1.295043706893921\n",
            "Epoch: 37 Step: 18600 Val Loss: 1.2924734354019165\n",
            "Epoch: 38 Step: 18625 Val Loss: 1.2953479290008545\n",
            "Epoch: 38 Step: 18650 Val Loss: 1.2978477478027344\n",
            "Epoch: 38 Step: 18675 Val Loss: 1.2999075651168823\n",
            "Epoch: 38 Step: 18700 Val Loss: 1.295061707496643\n",
            "Epoch: 38 Step: 18725 Val Loss: 1.2955195903778076\n",
            "Epoch: 38 Step: 18750 Val Loss: 1.2924937009811401\n",
            "Epoch: 38 Step: 18775 Val Loss: 1.2952985763549805\n",
            "Epoch: 38 Step: 18800 Val Loss: 1.2947545051574707\n",
            "Epoch: 38 Step: 18825 Val Loss: 1.292855978012085\n",
            "Epoch: 38 Step: 18850 Val Loss: 1.2954245805740356\n",
            "Epoch: 38 Step: 18875 Val Loss: 1.2931580543518066\n",
            "Epoch: 38 Step: 18900 Val Loss: 1.292331337928772\n",
            "Epoch: 38 Step: 18925 Val Loss: 1.2941181659698486\n",
            "Epoch: 38 Step: 18950 Val Loss: 1.2916029691696167\n",
            "Epoch: 38 Step: 18975 Val Loss: 1.3007231950759888\n",
            "Epoch: 38 Step: 19000 Val Loss: 1.2998497486114502\n",
            "Epoch: 38 Step: 19025 Val Loss: 1.301867127418518\n",
            "Epoch: 38 Step: 19050 Val Loss: 1.307133436203003\n",
            "Epoch: 38 Step: 19075 Val Loss: 1.3012734651565552\n",
            "Epoch: 38 Step: 19100 Val Loss: 1.3013911247253418\n",
            "Epoch: 39 Step: 19125 Val Loss: 1.2993888854980469\n",
            "Epoch: 39 Step: 19150 Val Loss: 1.2973506450653076\n",
            "Epoch: 39 Step: 19175 Val Loss: 1.3005667924880981\n",
            "Epoch: 39 Step: 19200 Val Loss: 1.2964781522750854\n",
            "Epoch: 39 Step: 19225 Val Loss: 1.2995195388793945\n",
            "Epoch: 39 Step: 19250 Val Loss: 1.298194169998169\n",
            "Epoch: 39 Step: 19275 Val Loss: 1.3022252321243286\n",
            "Epoch: 39 Step: 19300 Val Loss: 1.297345519065857\n",
            "Epoch: 39 Step: 19325 Val Loss: 1.2975066900253296\n",
            "Epoch: 39 Step: 19350 Val Loss: 1.2989261150360107\n",
            "Epoch: 39 Step: 19375 Val Loss: 1.2930020093917847\n",
            "Epoch: 39 Step: 19400 Val Loss: 1.2991238832473755\n",
            "Epoch: 39 Step: 19425 Val Loss: 1.2942463159561157\n",
            "Epoch: 39 Step: 19450 Val Loss: 1.2938778400421143\n",
            "Epoch: 39 Step: 19475 Val Loss: 1.2997562885284424\n",
            "Epoch: 39 Step: 19500 Val Loss: 1.3047449588775635\n",
            "Epoch: 39 Step: 19525 Val Loss: 1.302309513092041\n",
            "Epoch: 39 Step: 19550 Val Loss: 1.3062397241592407\n",
            "Epoch: 39 Step: 19575 Val Loss: 1.308138132095337\n",
            "Epoch: 39 Step: 19600 Val Loss: 1.301548719406128\n",
            "Epoch: 40 Step: 19625 Val Loss: 1.3001059293746948\n",
            "Epoch: 40 Step: 19650 Val Loss: 1.3018134832382202\n",
            "Epoch: 40 Step: 19675 Val Loss: 1.3011152744293213\n",
            "Epoch: 40 Step: 19700 Val Loss: 1.2962621450424194\n",
            "Epoch: 40 Step: 19725 Val Loss: 1.2942945957183838\n",
            "Epoch: 40 Step: 19750 Val Loss: 1.2964131832122803\n",
            "Epoch: 40 Step: 19775 Val Loss: 1.2951079607009888\n",
            "Epoch: 40 Step: 19800 Val Loss: 1.295806884765625\n",
            "Epoch: 40 Step: 19825 Val Loss: 1.2936811447143555\n",
            "Epoch: 40 Step: 19850 Val Loss: 1.2913600206375122\n",
            "Epoch: 40 Step: 19875 Val Loss: 1.292994737625122\n",
            "Epoch: 40 Step: 19900 Val Loss: 1.2929015159606934\n",
            "Epoch: 40 Step: 19925 Val Loss: 1.286899209022522\n",
            "Epoch: 40 Step: 19950 Val Loss: 1.2926812171936035\n",
            "Epoch: 40 Step: 19975 Val Loss: 1.297542929649353\n",
            "Epoch: 40 Step: 20000 Val Loss: 1.2989072799682617\n",
            "Epoch: 40 Step: 20025 Val Loss: 1.2937917709350586\n",
            "Epoch: 40 Step: 20050 Val Loss: 1.296604871749878\n",
            "Epoch: 40 Step: 20075 Val Loss: 1.294995665550232\n",
            "Epoch: 41 Step: 20100 Val Loss: 1.2983144521713257\n",
            "Epoch: 41 Step: 20125 Val Loss: 1.3011298179626465\n",
            "Epoch: 41 Step: 20150 Val Loss: 1.3043593168258667\n",
            "Epoch: 41 Step: 20175 Val Loss: 1.3023053407669067\n",
            "Epoch: 41 Step: 20200 Val Loss: 1.3018001317977905\n",
            "Epoch: 41 Step: 20225 Val Loss: 1.3022139072418213\n",
            "Epoch: 41 Step: 20250 Val Loss: 1.303142786026001\n",
            "Epoch: 41 Step: 20275 Val Loss: 1.3005837202072144\n",
            "Epoch: 41 Step: 20300 Val Loss: 1.2959896326065063\n",
            "Epoch: 41 Step: 20325 Val Loss: 1.2990756034851074\n",
            "Epoch: 41 Step: 20350 Val Loss: 1.2923176288604736\n",
            "Epoch: 41 Step: 20375 Val Loss: 1.2978631258010864\n",
            "Epoch: 41 Step: 20400 Val Loss: 1.3003945350646973\n",
            "Epoch: 41 Step: 20425 Val Loss: 1.2928316593170166\n",
            "Epoch: 41 Step: 20450 Val Loss: 1.29464590549469\n",
            "Epoch: 41 Step: 20475 Val Loss: 1.3008166551589966\n",
            "Epoch: 41 Step: 20500 Val Loss: 1.3042351007461548\n",
            "Epoch: 41 Step: 20525 Val Loss: 1.303266167640686\n",
            "Epoch: 41 Step: 20550 Val Loss: 1.3038864135742188\n",
            "Epoch: 41 Step: 20575 Val Loss: 1.3001729249954224\n",
            "Epoch: 42 Step: 20600 Val Loss: 1.3024165630340576\n",
            "Epoch: 42 Step: 20625 Val Loss: 1.3045413494110107\n",
            "Epoch: 42 Step: 20650 Val Loss: 1.302509069442749\n",
            "Epoch: 42 Step: 20675 Val Loss: 1.301851511001587\n",
            "Epoch: 42 Step: 20700 Val Loss: 1.3002032041549683\n",
            "Epoch: 42 Step: 20725 Val Loss: 1.299315333366394\n",
            "Epoch: 42 Step: 20750 Val Loss: 1.300839900970459\n",
            "Epoch: 42 Step: 20775 Val Loss: 1.2975976467132568\n",
            "Epoch: 42 Step: 20800 Val Loss: 1.2950345277786255\n",
            "Epoch: 42 Step: 20825 Val Loss: 1.2953675985336304\n",
            "Epoch: 42 Step: 20850 Val Loss: 1.3003225326538086\n",
            "Epoch: 42 Step: 20875 Val Loss: 1.297957181930542\n",
            "Epoch: 42 Step: 20900 Val Loss: 1.2932374477386475\n",
            "Epoch: 42 Step: 20925 Val Loss: 1.2912328243255615\n",
            "Epoch: 42 Step: 20950 Val Loss: 1.2994015216827393\n",
            "Epoch: 42 Step: 20975 Val Loss: 1.3047778606414795\n",
            "Epoch: 42 Step: 21000 Val Loss: 1.3061072826385498\n",
            "Epoch: 42 Step: 21025 Val Loss: 1.3043491840362549\n",
            "Epoch: 42 Step: 21050 Val Loss: 1.3013783693313599\n",
            "Epoch: 43 Step: 21075 Val Loss: 1.3006352186203003\n",
            "Epoch: 43 Step: 21100 Val Loss: 1.2997467517852783\n",
            "Epoch: 43 Step: 21125 Val Loss: 1.3049677610397339\n",
            "Epoch: 43 Step: 21150 Val Loss: 1.302830457687378\n",
            "Epoch: 43 Step: 21175 Val Loss: 1.3031154870986938\n",
            "Epoch: 43 Step: 21200 Val Loss: 1.3045158386230469\n",
            "Epoch: 43 Step: 21225 Val Loss: 1.2969820499420166\n",
            "Epoch: 43 Step: 21250 Val Loss: 1.299496054649353\n",
            "Epoch: 43 Step: 21275 Val Loss: 1.2988569736480713\n",
            "Epoch: 43 Step: 21300 Val Loss: 1.2975918054580688\n",
            "Epoch: 43 Step: 21325 Val Loss: 1.2981739044189453\n",
            "Epoch: 43 Step: 21350 Val Loss: 1.3020265102386475\n",
            "Epoch: 43 Step: 21375 Val Loss: 1.2979748249053955\n",
            "Epoch: 43 Step: 21400 Val Loss: 1.2947255373001099\n",
            "Epoch: 43 Step: 21425 Val Loss: 1.3037883043289185\n",
            "Epoch: 43 Step: 21450 Val Loss: 1.3038501739501953\n",
            "Epoch: 43 Step: 21475 Val Loss: 1.3027892112731934\n",
            "Epoch: 43 Step: 21500 Val Loss: 1.3083176612854004\n",
            "Epoch: 43 Step: 21525 Val Loss: 1.3045055866241455\n",
            "Epoch: 43 Step: 21550 Val Loss: 1.301764965057373\n",
            "Epoch: 44 Step: 21575 Val Loss: 1.3012827634811401\n",
            "Epoch: 44 Step: 21600 Val Loss: 1.2984751462936401\n",
            "Epoch: 44 Step: 21625 Val Loss: 1.2987005710601807\n",
            "Epoch: 44 Step: 21650 Val Loss: 1.2988635301589966\n",
            "Epoch: 44 Step: 21675 Val Loss: 1.299817442893982\n",
            "Epoch: 44 Step: 21700 Val Loss: 1.2988190650939941\n",
            "Epoch: 44 Step: 21725 Val Loss: 1.301879644393921\n",
            "Epoch: 44 Step: 21750 Val Loss: 1.2960405349731445\n",
            "Epoch: 44 Step: 21775 Val Loss: 1.2990714311599731\n",
            "Epoch: 44 Step: 21800 Val Loss: 1.2968485355377197\n",
            "Epoch: 44 Step: 21825 Val Loss: 1.2934197187423706\n",
            "Epoch: 44 Step: 21850 Val Loss: 1.2987279891967773\n",
            "Epoch: 44 Step: 21875 Val Loss: 1.2978543043136597\n",
            "Epoch: 44 Step: 21900 Val Loss: 1.2977054119110107\n",
            "Epoch: 44 Step: 21925 Val Loss: 1.303875207901001\n",
            "Epoch: 44 Step: 21950 Val Loss: 1.3010927438735962\n",
            "Epoch: 44 Step: 21975 Val Loss: 1.3025238513946533\n",
            "Epoch: 44 Step: 22000 Val Loss: 1.2984918355941772\n",
            "Epoch: 44 Step: 22025 Val Loss: 1.2995250225067139\n",
            "Epoch: 44 Step: 22050 Val Loss: 1.2983802556991577\n",
            "Epoch: 45 Step: 22075 Val Loss: 1.3001203536987305\n",
            "Epoch: 45 Step: 22100 Val Loss: 1.3048923015594482\n",
            "Epoch: 45 Step: 22125 Val Loss: 1.30280601978302\n",
            "Epoch: 45 Step: 22150 Val Loss: 1.3018344640731812\n",
            "Epoch: 45 Step: 22175 Val Loss: 1.29715895652771\n",
            "Epoch: 45 Step: 22200 Val Loss: 1.2995020151138306\n",
            "Epoch: 45 Step: 22225 Val Loss: 1.301066279411316\n",
            "Epoch: 45 Step: 22250 Val Loss: 1.299157977104187\n",
            "Epoch: 45 Step: 22275 Val Loss: 1.2971895933151245\n",
            "Epoch: 45 Step: 22300 Val Loss: 1.291690468788147\n",
            "Epoch: 45 Step: 22325 Val Loss: 1.2930330038070679\n",
            "Epoch: 45 Step: 22350 Val Loss: 1.292826533317566\n",
            "Epoch: 45 Step: 22375 Val Loss: 1.2885961532592773\n",
            "Epoch: 45 Step: 22400 Val Loss: 1.2967898845672607\n",
            "Epoch: 45 Step: 22425 Val Loss: 1.2954763174057007\n",
            "Epoch: 45 Step: 22450 Val Loss: 1.2970390319824219\n",
            "Epoch: 45 Step: 22475 Val Loss: 1.2967013120651245\n",
            "Epoch: 45 Step: 22500 Val Loss: 1.2972769737243652\n",
            "Epoch: 45 Step: 22525 Val Loss: 1.2983897924423218\n",
            "Epoch: 46 Step: 22550 Val Loss: 1.297453761100769\n",
            "Epoch: 46 Step: 22575 Val Loss: 1.2984938621520996\n",
            "Epoch: 46 Step: 22600 Val Loss: 1.300544023513794\n",
            "Epoch: 46 Step: 22625 Val Loss: 1.297727346420288\n",
            "Epoch: 46 Step: 22650 Val Loss: 1.2993097305297852\n",
            "Epoch: 46 Step: 22675 Val Loss: 1.2954221963882446\n",
            "Epoch: 46 Step: 22700 Val Loss: 1.2996463775634766\n",
            "Epoch: 46 Step: 22725 Val Loss: 1.2993706464767456\n",
            "Epoch: 46 Step: 22750 Val Loss: 1.2949637174606323\n",
            "Epoch: 46 Step: 22775 Val Loss: 1.2974234819412231\n",
            "Epoch: 46 Step: 22800 Val Loss: 1.2915834188461304\n",
            "Epoch: 46 Step: 22825 Val Loss: 1.2923592329025269\n",
            "Epoch: 46 Step: 22850 Val Loss: 1.2933553457260132\n",
            "Epoch: 46 Step: 22875 Val Loss: 1.2946008443832397\n",
            "Epoch: 46 Step: 22900 Val Loss: 1.2979825735092163\n",
            "Epoch: 46 Step: 22925 Val Loss: 1.303199291229248\n",
            "Epoch: 46 Step: 22950 Val Loss: 1.3033884763717651\n",
            "Epoch: 46 Step: 22975 Val Loss: 1.3075731992721558\n",
            "Epoch: 46 Step: 23000 Val Loss: 1.3094310760498047\n",
            "Epoch: 46 Step: 23025 Val Loss: 1.3017035722732544\n",
            "Epoch: 47 Step: 23050 Val Loss: 1.3058571815490723\n",
            "Epoch: 47 Step: 23075 Val Loss: 1.3052295446395874\n",
            "Epoch: 47 Step: 23100 Val Loss: 1.3027503490447998\n",
            "Epoch: 47 Step: 23125 Val Loss: 1.3021888732910156\n",
            "Epoch: 47 Step: 23150 Val Loss: 1.3058680295944214\n",
            "Epoch: 47 Step: 23175 Val Loss: 1.2983919382095337\n",
            "Epoch: 47 Step: 23200 Val Loss: 1.300726294517517\n",
            "Epoch: 47 Step: 23225 Val Loss: 1.2969319820404053\n",
            "Epoch: 47 Step: 23250 Val Loss: 1.294313907623291\n",
            "Epoch: 47 Step: 23275 Val Loss: 1.2956258058547974\n",
            "Epoch: 47 Step: 23300 Val Loss: 1.295267939567566\n",
            "Epoch: 47 Step: 23325 Val Loss: 1.3022652864456177\n",
            "Epoch: 47 Step: 23350 Val Loss: 1.2945653200149536\n",
            "Epoch: 47 Step: 23375 Val Loss: 1.2963509559631348\n",
            "Epoch: 47 Step: 23400 Val Loss: 1.3007524013519287\n",
            "Epoch: 47 Step: 23425 Val Loss: 1.30060875415802\n",
            "Epoch: 47 Step: 23450 Val Loss: 1.3008309602737427\n",
            "Epoch: 47 Step: 23475 Val Loss: 1.3029085397720337\n",
            "Epoch: 47 Step: 23500 Val Loss: 1.2982501983642578\n",
            "Epoch: 48 Step: 23525 Val Loss: 1.3004218339920044\n",
            "Epoch: 48 Step: 23550 Val Loss: 1.3007714748382568\n",
            "Epoch: 48 Step: 23575 Val Loss: 1.302519679069519\n",
            "Epoch: 48 Step: 23600 Val Loss: 1.2991329431533813\n",
            "Epoch: 48 Step: 23625 Val Loss: 1.2994736433029175\n",
            "Epoch: 48 Step: 23650 Val Loss: 1.3046343326568604\n",
            "Epoch: 48 Step: 23675 Val Loss: 1.2942733764648438\n",
            "Epoch: 48 Step: 23700 Val Loss: 1.293103575706482\n",
            "Epoch: 48 Step: 23725 Val Loss: 1.2970725297927856\n",
            "Epoch: 48 Step: 23750 Val Loss: 1.2926087379455566\n",
            "Epoch: 48 Step: 23775 Val Loss: 1.290963888168335\n",
            "Epoch: 48 Step: 23800 Val Loss: 1.2934162616729736\n",
            "Epoch: 48 Step: 23825 Val Loss: 1.293487310409546\n",
            "Epoch: 48 Step: 23850 Val Loss: 1.2947039604187012\n",
            "Epoch: 48 Step: 23875 Val Loss: 1.3005839586257935\n",
            "Epoch: 48 Step: 23900 Val Loss: 1.2951281070709229\n",
            "Epoch: 48 Step: 23925 Val Loss: 1.293868899345398\n",
            "Epoch: 48 Step: 23950 Val Loss: 1.3014774322509766\n",
            "Epoch: 48 Step: 23975 Val Loss: 1.3005489110946655\n",
            "Epoch: 48 Step: 24000 Val Loss: 1.2982124090194702\n",
            "Epoch: 49 Step: 24025 Val Loss: 1.2964909076690674\n",
            "Epoch: 49 Step: 24050 Val Loss: 1.2952576875686646\n",
            "Epoch: 49 Step: 24075 Val Loss: 1.3005030155181885\n",
            "Epoch: 49 Step: 24100 Val Loss: 1.298231840133667\n",
            "Epoch: 49 Step: 24125 Val Loss: 1.2994011640548706\n",
            "Epoch: 49 Step: 24150 Val Loss: 1.2943673133850098\n",
            "Epoch: 49 Step: 24175 Val Loss: 1.2967504262924194\n",
            "Epoch: 49 Step: 24200 Val Loss: 1.2961223125457764\n",
            "Epoch: 49 Step: 24225 Val Loss: 1.2914615869522095\n",
            "Epoch: 49 Step: 24250 Val Loss: 1.2927969694137573\n",
            "Epoch: 49 Step: 24275 Val Loss: 1.2904274463653564\n",
            "Epoch: 49 Step: 24300 Val Loss: 1.294368863105774\n",
            "Epoch: 49 Step: 24325 Val Loss: 1.2957510948181152\n",
            "Epoch: 49 Step: 24350 Val Loss: 1.2946810722351074\n",
            "Epoch: 49 Step: 24375 Val Loss: 1.3039692640304565\n",
            "Epoch: 49 Step: 24400 Val Loss: 1.3044852018356323\n",
            "Epoch: 49 Step: 24425 Val Loss: 1.2992340326309204\n",
            "Epoch: 49 Step: 24450 Val Loss: 1.2974779605865479\n",
            "Epoch: 49 Step: 24475 Val Loss: 1.3012256622314453\n",
            "Epoch: 49 Step: 24500 Val Loss: 1.2975046634674072\n",
            "Epoch: 50 Step: 24525 Val Loss: 1.298296332359314\n",
            "Epoch: 50 Step: 24550 Val Loss: 1.299309253692627\n",
            "Epoch: 50 Step: 24575 Val Loss: 1.2982509136199951\n",
            "Epoch: 50 Step: 24600 Val Loss: 1.3011606931686401\n",
            "Epoch: 50 Step: 24625 Val Loss: 1.2944397926330566\n",
            "Epoch: 50 Step: 24650 Val Loss: 1.2961045503616333\n",
            "Epoch: 50 Step: 24675 Val Loss: 1.2982724905014038\n",
            "Epoch: 50 Step: 24700 Val Loss: 1.297482967376709\n",
            "Epoch: 50 Step: 24725 Val Loss: 1.2970222234725952\n",
            "Epoch: 50 Step: 24750 Val Loss: 1.290962815284729\n",
            "Epoch: 50 Step: 24775 Val Loss: 1.2954031229019165\n",
            "Epoch: 50 Step: 24800 Val Loss: 1.2926063537597656\n",
            "Epoch: 50 Step: 24825 Val Loss: 1.2939320802688599\n",
            "Epoch: 50 Step: 24850 Val Loss: 1.292789340019226\n",
            "Epoch: 50 Step: 24875 Val Loss: 1.297415018081665\n",
            "Epoch: 50 Step: 24900 Val Loss: 1.3012601137161255\n",
            "Epoch: 50 Step: 24925 Val Loss: 1.3026213645935059\n",
            "Epoch: 50 Step: 24950 Val Loss: 1.297404408454895\n",
            "Epoch: 50 Step: 24975 Val Loss: 1.2927353382110596\n",
            "Epoch: 51 Step: 25000 Val Loss: 1.2976919412612915\n",
            "Epoch: 51 Step: 25025 Val Loss: 1.3049813508987427\n",
            "Epoch: 51 Step: 25050 Val Loss: 1.3052927255630493\n",
            "Epoch: 51 Step: 25075 Val Loss: 1.3058778047561646\n",
            "Epoch: 51 Step: 25100 Val Loss: 1.3055006265640259\n",
            "Epoch: 51 Step: 25125 Val Loss: 1.3005359172821045\n",
            "Epoch: 51 Step: 25150 Val Loss: 1.2984944581985474\n",
            "Epoch: 51 Step: 25175 Val Loss: 1.3004711866378784\n",
            "Epoch: 51 Step: 25200 Val Loss: 1.2999383211135864\n",
            "Epoch: 51 Step: 25225 Val Loss: 1.3001728057861328\n",
            "Epoch: 51 Step: 25250 Val Loss: 1.294054388999939\n",
            "Epoch: 51 Step: 25275 Val Loss: 1.2919481992721558\n",
            "Epoch: 51 Step: 25300 Val Loss: 1.2931097745895386\n",
            "Epoch: 51 Step: 25325 Val Loss: 1.291792631149292\n",
            "Epoch: 51 Step: 25350 Val Loss: 1.2968641519546509\n",
            "Epoch: 51 Step: 25375 Val Loss: 1.2986738681793213\n",
            "Epoch: 51 Step: 25400 Val Loss: 1.2996280193328857\n",
            "Epoch: 51 Step: 25425 Val Loss: 1.3050811290740967\n",
            "Epoch: 51 Step: 25450 Val Loss: 1.3046687841415405\n",
            "Epoch: 51 Step: 25475 Val Loss: 1.2987648248672485\n",
            "Epoch: 52 Step: 25500 Val Loss: 1.2979010343551636\n",
            "Epoch: 52 Step: 25525 Val Loss: 1.302574634552002\n",
            "Epoch: 52 Step: 25550 Val Loss: 1.294974684715271\n",
            "Epoch: 52 Step: 25575 Val Loss: 1.297446846961975\n",
            "Epoch: 52 Step: 25600 Val Loss: 1.2996246814727783\n",
            "Epoch: 52 Step: 25625 Val Loss: 1.2943780422210693\n",
            "Epoch: 52 Step: 25650 Val Loss: 1.2957624197006226\n",
            "Epoch: 52 Step: 25675 Val Loss: 1.2913966178894043\n",
            "Epoch: 52 Step: 25700 Val Loss: 1.2869503498077393\n",
            "Epoch: 52 Step: 25725 Val Loss: 1.2904798984527588\n",
            "Epoch: 52 Step: 25750 Val Loss: 1.292476773262024\n",
            "Epoch: 52 Step: 25775 Val Loss: 1.293962836265564\n",
            "Epoch: 52 Step: 25800 Val Loss: 1.288150668144226\n",
            "Epoch: 52 Step: 25825 Val Loss: 1.2924288511276245\n",
            "Epoch: 52 Step: 25850 Val Loss: 1.292685627937317\n",
            "Epoch: 52 Step: 25875 Val Loss: 1.2912681102752686\n",
            "Epoch: 52 Step: 25900 Val Loss: 1.2913329601287842\n",
            "Epoch: 52 Step: 25925 Val Loss: 1.2931164503097534\n",
            "Epoch: 52 Step: 25950 Val Loss: 1.2925806045532227\n",
            "Epoch: 53 Step: 25975 Val Loss: 1.2945375442504883\n",
            "Epoch: 53 Step: 26000 Val Loss: 1.2932308912277222\n",
            "Epoch: 53 Step: 26025 Val Loss: 1.297608733177185\n",
            "Epoch: 53 Step: 26050 Val Loss: 1.2964304685592651\n",
            "Epoch: 53 Step: 26075 Val Loss: 1.296044111251831\n",
            "Epoch: 53 Step: 26100 Val Loss: 1.295872449874878\n",
            "Epoch: 53 Step: 26125 Val Loss: 1.2935625314712524\n",
            "Epoch: 53 Step: 26150 Val Loss: 1.2904555797576904\n",
            "Epoch: 53 Step: 26175 Val Loss: 1.2888264656066895\n",
            "Epoch: 53 Step: 26200 Val Loss: 1.2917805910110474\n",
            "Epoch: 53 Step: 26225 Val Loss: 1.2875670194625854\n",
            "Epoch: 53 Step: 26250 Val Loss: 1.2909754514694214\n",
            "Epoch: 53 Step: 26275 Val Loss: 1.2886775732040405\n",
            "Epoch: 53 Step: 26300 Val Loss: 1.288066029548645\n",
            "Epoch: 53 Step: 26325 Val Loss: 1.289078950881958\n",
            "Epoch: 53 Step: 26350 Val Loss: 1.294464349746704\n",
            "Epoch: 53 Step: 26375 Val Loss: 1.2910622358322144\n",
            "Epoch: 53 Step: 26400 Val Loss: 1.296567440032959\n",
            "Epoch: 53 Step: 26425 Val Loss: 1.294805884361267\n",
            "Epoch: 53 Step: 26450 Val Loss: 1.2976877689361572\n",
            "Epoch: 54 Step: 26475 Val Loss: 1.2976919412612915\n",
            "Epoch: 54 Step: 26500 Val Loss: 1.2944241762161255\n",
            "Epoch: 54 Step: 26525 Val Loss: 1.2931917905807495\n",
            "Epoch: 54 Step: 26550 Val Loss: 1.2950336933135986\n",
            "Epoch: 54 Step: 26575 Val Loss: 1.2971346378326416\n",
            "Epoch: 54 Step: 26600 Val Loss: 1.291916847229004\n",
            "Epoch: 54 Step: 26625 Val Loss: 1.2962076663970947\n",
            "Epoch: 54 Step: 26650 Val Loss: 1.294536828994751\n",
            "Epoch: 54 Step: 26675 Val Loss: 1.2928229570388794\n",
            "Epoch: 54 Step: 26700 Val Loss: 1.2988898754119873\n",
            "Epoch: 54 Step: 26725 Val Loss: 1.2954758405685425\n",
            "Epoch: 54 Step: 26750 Val Loss: 1.2989449501037598\n",
            "Epoch: 54 Step: 26775 Val Loss: 1.293226957321167\n",
            "Epoch: 54 Step: 26800 Val Loss: 1.2919243574142456\n",
            "Epoch: 54 Step: 26825 Val Loss: 1.2931509017944336\n",
            "Epoch: 54 Step: 26850 Val Loss: 1.2960245609283447\n",
            "Epoch: 54 Step: 26875 Val Loss: 1.2992987632751465\n",
            "Epoch: 54 Step: 26900 Val Loss: 1.2971161603927612\n",
            "Epoch: 54 Step: 26925 Val Loss: 1.299277901649475\n",
            "Epoch: 54 Step: 26950 Val Loss: 1.2962477207183838\n",
            "Epoch: 55 Step: 26975 Val Loss: 1.2972559928894043\n",
            "Epoch: 55 Step: 27000 Val Loss: 1.297743320465088\n",
            "Epoch: 55 Step: 27025 Val Loss: 1.3022441864013672\n",
            "Epoch: 55 Step: 27050 Val Loss: 1.3012028932571411\n",
            "Epoch: 55 Step: 27075 Val Loss: 1.2983266115188599\n",
            "Epoch: 55 Step: 27100 Val Loss: 1.2930055856704712\n",
            "Epoch: 55 Step: 27125 Val Loss: 1.2976384162902832\n",
            "Epoch: 55 Step: 27150 Val Loss: 1.301236867904663\n",
            "Epoch: 55 Step: 27175 Val Loss: 1.2975952625274658\n",
            "Epoch: 55 Step: 27200 Val Loss: 1.2929327487945557\n",
            "Epoch: 55 Step: 27225 Val Loss: 1.2969526052474976\n",
            "Epoch: 55 Step: 27250 Val Loss: 1.2946807146072388\n",
            "Epoch: 55 Step: 27275 Val Loss: 1.289345622062683\n",
            "Epoch: 55 Step: 27300 Val Loss: 1.2950024604797363\n",
            "Epoch: 55 Step: 27325 Val Loss: 1.2970932722091675\n",
            "Epoch: 55 Step: 27350 Val Loss: 1.2925639152526855\n",
            "Epoch: 55 Step: 27375 Val Loss: 1.2940939664840698\n",
            "Epoch: 55 Step: 27400 Val Loss: 1.2936123609542847\n",
            "Epoch: 55 Step: 27425 Val Loss: 1.2903894186019897\n",
            "Epoch: 56 Step: 27450 Val Loss: 1.2932218313217163\n",
            "Epoch: 56 Step: 27475 Val Loss: 1.298000693321228\n",
            "Epoch: 56 Step: 27500 Val Loss: 1.2989803552627563\n",
            "Epoch: 56 Step: 27525 Val Loss: 1.297316551208496\n",
            "Epoch: 56 Step: 27550 Val Loss: 1.3000624179840088\n",
            "Epoch: 56 Step: 27575 Val Loss: 1.294014573097229\n",
            "Epoch: 56 Step: 27600 Val Loss: 1.2938908338546753\n",
            "Epoch: 56 Step: 27625 Val Loss: 1.2893733978271484\n",
            "Epoch: 56 Step: 27650 Val Loss: 1.2876628637313843\n",
            "Epoch: 56 Step: 27675 Val Loss: 1.2912062406539917\n",
            "Epoch: 56 Step: 27700 Val Loss: 1.2923351526260376\n",
            "Epoch: 56 Step: 27725 Val Loss: 1.290507435798645\n",
            "Epoch: 56 Step: 27750 Val Loss: 1.2925529479980469\n",
            "Epoch: 56 Step: 27775 Val Loss: 1.2910170555114746\n",
            "Epoch: 56 Step: 27800 Val Loss: 1.2899150848388672\n",
            "Epoch: 56 Step: 27825 Val Loss: 1.2895591259002686\n",
            "Epoch: 56 Step: 27850 Val Loss: 1.2923353910446167\n",
            "Epoch: 56 Step: 27875 Val Loss: 1.296127200126648\n",
            "Epoch: 56 Step: 27900 Val Loss: 1.295501470565796\n",
            "Epoch: 56 Step: 27925 Val Loss: 1.2905634641647339\n",
            "Epoch: 57 Step: 27950 Val Loss: 1.2915067672729492\n",
            "Epoch: 57 Step: 27975 Val Loss: 1.293178915977478\n",
            "Epoch: 57 Step: 28000 Val Loss: 1.2921069860458374\n",
            "Epoch: 57 Step: 28025 Val Loss: 1.2917331457138062\n",
            "Epoch: 57 Step: 28050 Val Loss: 1.2928580045700073\n",
            "Epoch: 57 Step: 28075 Val Loss: 1.2854334115982056\n",
            "Epoch: 57 Step: 28100 Val Loss: 1.2866289615631104\n",
            "Epoch: 57 Step: 28125 Val Loss: 1.2871928215026855\n",
            "Epoch: 57 Step: 28150 Val Loss: 1.2890739440917969\n",
            "Epoch: 57 Step: 28175 Val Loss: 1.290695071220398\n",
            "Epoch: 57 Step: 28200 Val Loss: 1.2877581119537354\n",
            "Epoch: 57 Step: 28225 Val Loss: 1.2867681980133057\n",
            "Epoch: 57 Step: 28250 Val Loss: 1.2867451906204224\n",
            "Epoch: 57 Step: 28275 Val Loss: 1.2889186143875122\n",
            "Epoch: 57 Step: 28300 Val Loss: 1.2968709468841553\n",
            "Epoch: 57 Step: 28325 Val Loss: 1.2952224016189575\n",
            "Epoch: 57 Step: 28350 Val Loss: 1.3831907510757446\n",
            "Epoch: 57 Step: 28375 Val Loss: 1.3105601072311401\n",
            "Epoch: 57 Step: 28400 Val Loss: 1.2940189838409424\n",
            "Epoch: 58 Step: 28425 Val Loss: 1.2880065441131592\n",
            "Epoch: 58 Step: 28450 Val Loss: 1.2852004766464233\n",
            "Epoch: 58 Step: 28475 Val Loss: 1.2831937074661255\n",
            "Epoch: 58 Step: 28500 Val Loss: 1.2805721759796143\n",
            "Epoch: 58 Step: 28525 Val Loss: 1.2827634811401367\n",
            "Epoch: 58 Step: 28550 Val Loss: 1.2813366651535034\n",
            "Epoch: 58 Step: 28575 Val Loss: 1.2819609642028809\n",
            "Epoch: 58 Step: 28600 Val Loss: 1.2847731113433838\n",
            "Epoch: 58 Step: 28625 Val Loss: 1.2860164642333984\n",
            "Epoch: 58 Step: 28650 Val Loss: 1.282940149307251\n",
            "Epoch: 58 Step: 28675 Val Loss: 1.2811801433563232\n",
            "Epoch: 58 Step: 28700 Val Loss: 1.285094141960144\n",
            "Epoch: 58 Step: 28725 Val Loss: 1.2860599756240845\n",
            "Epoch: 58 Step: 28750 Val Loss: 1.2808973789215088\n",
            "Epoch: 58 Step: 28775 Val Loss: 1.2856251001358032\n",
            "Epoch: 58 Step: 28800 Val Loss: 1.2860944271087646\n",
            "Epoch: 58 Step: 28825 Val Loss: 1.2868964672088623\n",
            "Epoch: 58 Step: 28850 Val Loss: 1.2880122661590576\n",
            "Epoch: 58 Step: 28875 Val Loss: 1.2892874479293823\n",
            "Epoch: 58 Step: 28900 Val Loss: 1.290442943572998\n",
            "Epoch: 59 Step: 28925 Val Loss: 1.2890865802764893\n",
            "Epoch: 59 Step: 28950 Val Loss: 1.2880300283432007\n",
            "Epoch: 59 Step: 28975 Val Loss: 1.2868291139602661\n",
            "Epoch: 59 Step: 29000 Val Loss: 1.2882487773895264\n",
            "Epoch: 59 Step: 29025 Val Loss: 1.2916057109832764\n",
            "Epoch: 59 Step: 29050 Val Loss: 1.2888731956481934\n",
            "Epoch: 59 Step: 29075 Val Loss: 1.2878694534301758\n",
            "Epoch: 59 Step: 29100 Val Loss: 1.2875608205795288\n",
            "Epoch: 59 Step: 29125 Val Loss: 1.2888829708099365\n",
            "Epoch: 59 Step: 29150 Val Loss: 1.2913953065872192\n",
            "Epoch: 59 Step: 29175 Val Loss: 1.2872916460037231\n",
            "Epoch: 59 Step: 29200 Val Loss: 1.2888668775558472\n",
            "Epoch: 59 Step: 29225 Val Loss: 1.2891011238098145\n",
            "Epoch: 59 Step: 29250 Val Loss: 1.2891924381256104\n",
            "Epoch: 59 Step: 29275 Val Loss: 1.2922707796096802\n",
            "Epoch: 59 Step: 29300 Val Loss: 1.2943949699401855\n",
            "Epoch: 59 Step: 29325 Val Loss: 1.2956860065460205\n",
            "Epoch: 59 Step: 29350 Val Loss: 1.2942615747451782\n",
            "Epoch: 59 Step: 29375 Val Loss: 1.2941566705703735\n",
            "Epoch: 59 Step: 29400 Val Loss: 1.2886360883712769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM5d-5ifGSJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'Hidden512_layers_3_shakes.net'"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udbFAxykIpvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),model_name)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV4bbSDFIurf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model,char,hidden=None,k=1):\n",
        "\n",
        "  encoded_text = model.encoder[char]\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  encoded_text = one_hot_encoder(encoded_text,len(model.all_chars))\n",
        "\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "  if model.use_gpu:\n",
        "    inputs = inputs.cuda()\n",
        "\n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "  lstm_out,hidden = model(inputs,hidden)\n",
        "  \n",
        "  probs = F.softmax(lstm_out,dim=1).data\n",
        "\n",
        "  if model.use_gpu:\n",
        "    probs = probs.cpu()\n",
        "\n",
        "  probs,index_positions = probs.topk(k)\n",
        "\n",
        "  index_positions = index_positions_numpy().squeeze()\n",
        "\n",
        "  probs = probs.numpy().flatten()\n",
        "\n",
        "  probs = probs/probs.sum()\n",
        "\n",
        "  char = np.random.choice(index_positions,p=probs)\n",
        "\n",
        "  return model.decoder[char],hidden"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBM4kRGHgeYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfC3lqPTgev6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e4e3ad18-0e06-45ef-b805-a480fa729186"
      },
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-3atd99g4BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "        # Encode raw letters with model\n",
        "        encoded_text = model.encoder[char]\n",
        "        \n",
        "        # set as numpy array for one hot encoding\n",
        "        # NOTE THE [[ ]] dimensions!!\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "        \n",
        "        # One hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "        \n",
        "        # Convert to Tensor\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "        \n",
        "        # Check for CPU\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        \n",
        "        # Grab hidden states\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        \n",
        "        # Run model and get predicted output\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "        \n",
        "        # Convert lstm_out to probabilities\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "        \n",
        "        \n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            # move back to CPU to use with numpy\n",
        "            probs = probs.cpu()\n",
        "        \n",
        "        \n",
        "        # k determines how many characters to consider\n",
        "        # for our probability choice.\n",
        "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "        \n",
        "        # Return k largest probabilities in tensor\n",
        "        probs, index_positions = probs.topk(k)\n",
        "        \n",
        "        \n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "        \n",
        "        # Create array of probabilities\n",
        "        probs = probs.numpy().flatten()\n",
        "        \n",
        "        # Convert to probabilities per index\n",
        "        probs = probs/probs.sum()\n",
        "        \n",
        "        # randomly choose a character based on probabilities\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "       \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.decoder[char], hidden"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00Cq4S3gxYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "        \n",
        "      \n",
        "    \n",
        "    # CHECK FOR GPU\n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "    \n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # begin output from initial seed\n",
        "    output_chars = [c for c in seed]\n",
        "    \n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "    \n",
        "    # predict the next character for every character in seed\n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "    \n",
        "    # add initial characters to output\n",
        "    output_chars.append(char)\n",
        "    \n",
        "    # Now generate for size requested\n",
        "    for i in range(size):\n",
        "        \n",
        "        # predict based off very last letter in output_chars\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "        \n",
        "        # add predicted character\n",
        "        output_chars.append(char)\n",
        "    \n",
        "    # return string of predicted text\n",
        "    return ''.join(output_chars)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GD9RLROgf46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "51fa11b8-217c-4c02-daff-ff2bfef31b20"
      },
      "source": [
        "print(generate_text(model, 1000, seed='The ', k=3))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The CONSTANCE OF WILLIAM\n",
            "SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\n",
            "PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\n",
            "WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\n",
            "DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\n",
            "PERSONILLALY OR YOUR OR OTHERS\n",
            "PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTION INCLUDES BY ANY BE\n",
            "DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\n",
            "PERSONAL USE ONLY, ARD (2) ARE FOR YOUR OR OTHERS\n",
            "PERSONAL USE OLLY, AND (2) ARE NOT DISTRIBUTED OR USED\n",
            "COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\n",
            "SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\n",
            "\n",
            "\n",
            " ANT II. Some come to hear his sons.\n",
            "  ANGELO, the Duke of Winchester. Though I was\n",
            "    many merchant that stands to my carriage, where to be\n",
            "    the child o' th' witch.\n",
            "  CORIOLANUS. It is not a good fool. If there be such a fine\n",
            "    That I was sold as I have sent me to me, I will see your\n",
            "    mind.\n",
            "  FIRST CITIZEN. The serv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}